{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "* Step 1: prepare the training and evaluation data set\n",
    "* Big change: I will compute the number of visits per week day\n",
    "* TO DO: apply a decreasing factor to weight down the older data\n",
    "* TO DO: After observing that individual restaurant reservations data are too noisy, I will try to add features based on overall reservations statistics on the restaurant city (area is too sparse) (including only HPG stores for now)\n",
    "\n",
    "from https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Warning: hpg_reserve must be dezipped first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load all Files (hey must be in input directory in a brother directory of the notebook)\n",
    "data_load = {\n",
    "    'air_reserve': pd.read_csv('../input/air_reserve.csv',parse_dates=['visit_datetime','reserve_datetime']), \n",
    "    'hpg_reserve': pd.read_csv('../input/hpg_reserve.csv',parse_dates=['visit_datetime','reserve_datetime']), \n",
    "    'air_store': pd.read_csv('../input/air_store_info.csv'),\n",
    "    'hpg_store': pd.read_csv('../input/hpg_store_info.csv'),\n",
    "    'air_visit': pd.read_csv('../input/air_visit_data.csv',parse_dates=['visit_date']),\n",
    "    'store_id': pd.read_csv('../input/store_id_relation.csv'),\n",
    "    'sample_sub': pd.read_csv('../input/sample_submission.csv'),\n",
    "    'holiday_dates': pd.read_csv('../input/date_info.csv',parse_dates=['calendar_date']).rename(columns={'calendar_date':'visit_date'})\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data = {}\n",
    "\n",
    "# Visits data (only for air site)\n",
    "Data['visit'] = data_load['air_visit'].copy()\n",
    "\n",
    "Data['visit']['DoW'] = Data['visit']['visit_date'].dt.dayofweek\n",
    "Data['visit']['visit_date'] = Data['visit']['visit_date'].apply(lambda d: d.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the submission set\n",
    "\n",
    "Data['submission'] = data_load['sample_sub'].copy()\n",
    "\n",
    "Data['submission']['air_store_id'] = Data['submission']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "Data['submission']['visit_date'] = Data['submission']['id'].map(lambda x: str(x).split('_')[2])\n",
    "Data['submission']['visit_date'] = pd.to_datetime(Data['submission']['visit_date'])\n",
    "Data['submission']['DoW'] = Data['submission']['visit_date'].dt.dayofweek\n",
    "Data['submission']['visit_date'] = Data['submission']['visit_date'].apply(lambda d:d.date())\n",
    "\n",
    "# Calculate the visit statistics per Day of Week, for all restaurant of the submission set\n",
    "\n",
    "unique_stores = Data['submission']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'DoW': \n",
    "    [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "tmp = Data['visit'].groupby(['air_store_id','DoW'], as_index=False)[\n",
    "    'visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','DoW']) \n",
    "tmp = Data['visit'].groupby(['air_store_id','DoW'], as_index=False)[\n",
    "    'visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','DoW'])\n",
    "tmp = Data['visit'].groupby(['air_store_id','DoW'], as_index=False)[\n",
    "    'visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','DoW'])\n",
    "tmp = Data['visit'].groupby(['air_store_id','DoW'], as_index=False)[\n",
    "    'visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','DoW'])\n",
    "tmp = Data['visit'].groupby(['air_store_id','DoW'], as_index=False)[\n",
    "    'visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','DoW']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the store information and one_hot encode\n",
    "stores = pd.merge(stores, data_load['air_store'], how='left', on=['air_store_id'])\n",
    "# stores = pd.get_dummies(stores, columns=['air_genre_name', 'air_area_name'])\n",
    "stores = pd.get_dummies(stores, columns=['air_genre_name'])\n",
    "\n",
    "stores['city'] = stores['air_area_name'].apply(lambda x: x.split(' ')[1])\n",
    "stores['prefecture'] = stores['air_area_name'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "train = pd.merge(Data['visit'], stores, how='left', on=['air_store_id','DoW']) \n",
    "test = pd.merge(Data['submission'], stores, how='left', on=['air_store_id','DoW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the day information\n",
    "ref_date = datetime.datetime(2017, 5, 31)\n",
    "Data['holidays'] = data_load['holiday_dates'].copy()\n",
    "\n",
    "# Adding features to identify periodicity and lagged effect\n",
    "Data['holidays']['visit_date'] = pd.to_datetime(Data['holidays']['visit_date'])\n",
    "Data['holidays']['delta_days'] = Data['holidays'].visit_date.apply(lambda d: (ref_date -d).days)\n",
    "\n",
    "Data['holidays']['month_cos'] = Data['holidays']['visit_date'].dt.month.apply(lambda m: np.cos(m*2*np.pi/12))\n",
    "Data['holidays']['month_sin'] = Data['holidays']['visit_date'].dt.month.apply(lambda m: np.sin(m*np.pi/12))\n",
    "\n",
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "\n",
    "wkend_holidays = Data['holidays'].apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "Data['holidays'].loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "\n",
    "Data['holidays']['visit_date'] =  Data['holidays']['visit_date'].dt.date\n",
    "\n",
    "Data['holidays'] = pd.get_dummies(Data['holidays'], columns=['day_of_week'])\n",
    "\n",
    "Data['visit'] = pd.merge(Data['visit'], Data['holidays'], how='left', on=['visit_date'])  \n",
    "Data['submission'] = pd.merge(Data['submission'], Data['holidays'], how='left', on=['visit_date'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the missing value with stats on all week days\n",
    "\n",
    "stat_visit_store = Data['visit'].groupby(['air_store_id'], as_index=False).visitors.agg({\n",
    "                                    'overall_mean_visitors': np.mean,\n",
    "                                    'overall_median_visitors': np.median,\n",
    "                                    'overall_count_observations': np.count_nonzero,\n",
    "                                    'overall_max_visitors': np.max,\n",
    "                                    'overall_min_visitors': np.min\n",
    "                                  })\n",
    "test = test.merge(stat_visit_store, \n",
    "                 how = 'left',\n",
    "                 on = 'air_store_id')\n",
    "\n",
    "for col in ['min_visitors',\n",
    "          'mean_visitors',\n",
    "          'median_visitors',\n",
    "          'max_visitors',\n",
    "          'count_observations']:\n",
    "    test.loc[test[col].isnull(), col]  = test.loc[test[col].isnull(), 'overall_' + col]\n",
    "    test = test.drop(['overall_' + col], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To include the reservations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Air and HPG sites reservations data\n",
    "# partially from https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "\n",
    "Data['reserve_air'] = data_load['air_reserve'].copy()\n",
    "Data['reserve_hpg'] = data_load['hpg_reserve'].copy()\n",
    "\n",
    "#Data['reserve_hpg'] = pd.merge(Data['reserve_hpg'], data_load['store_id'], how='left', on=['hpg_store_id'])\n",
    "\n",
    "for site in ['air', 'hpg']:\n",
    "\n",
    "    df = 'reserve_'+site\n",
    "    # Add the store information\n",
    "    \n",
    "    Data[df]['visit_date'] = Data[df]['visit_datetime'].apply(lambda d: d.date())\n",
    "    # Data[df]['reserve_date'] = Data[df]['reserve_datetime'].apply(lambda d: d.date()) \n",
    "    # Data[df]['reserve_date_diff'] = Data[df].apply(\n",
    "    #    lambda r: (r['visit_date'] - r['reserve_date']).days, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for site in ['air', 'hpg']:\n",
    "    \n",
    "    df = 'reserve_'+site\n",
    "    \n",
    "    # Calculate the total of reservations per day for restaurants with reservations in both sites \n",
    "    Data[df] = Data[df].groupby([site + '_store_id','visit_date'],\n",
    "                                as_index=False).reserve_visitors.sum().rename(columns={'reserve_visitors':'res_date_sum'})\n",
    "    Data[df]['DoW'] = Data[df]['visit_date'].apply(lambda d: d.weekday())\n",
    "    \n",
    "    # Add store data\n",
    "    Data[df] = pd.merge(Data[df], \n",
    "                        data_load[site +'_store'], \n",
    "                        how='inner', # Not all hpg store are described in hpg_store!!!\n",
    "                        on=[site + '_store_id'])\n",
    "    \n",
    "    Data[df]['city'] = Data[df][site + '_area_name'].apply(lambda x: x.split(' ')[1])\n",
    "    Data[df]['prefecture'] = Data[df][site + '_area_name'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    # Calculate the total of reservations per day of week for restaurants with reservations in both sites \n",
    "    tmp = Data[df].groupby([site+'_store_id', 'DoW'], as_index=False).res_date_sum.agg({'res_store_DoW_mean': np.mean})\n",
    "    \n",
    "    Data[df] = Data[df].merge(tmp,\n",
    "                              how = 'left',\n",
    "                              on = [site+'_store_id','DoW']\n",
    "                             )\n",
    "    \n",
    "    if (site == 'hpg'):\n",
    "        Data['reserve_hpg'] = pd.merge(Data['reserve_hpg'], data_load['store_id'], how='left', on=['hpg_store_id'])\n",
    "    \n",
    "    Data[df]['res_intensity'] = Data[df].res_date_sum / Data[df].res_store_DoW_mean \n",
    "    \n",
    "    # Add the information for recorded visits on air restaurants, from both sites\n",
    "    train = pd.merge(train, Data[df][['air_store_id','visit_date','res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['air_store_id','visit_date']).rename(columns={'res_intensity':'res_intensity_'+ site })\n",
    "    test = pd.merge(test, Data[df][['air_store_id','visit_date','res_intensity']], \n",
    "                    how='left', \n",
    "                    on=['air_store_id','visit_date']).rename(columns={'res_intensity':'res_intensity_'+ site })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manage the missing (store, date) reservations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in ['air', 'hpg']:\n",
    "       \n",
    "    # Calculate the mean of total reservations per date for each municipality, for both sites\n",
    "    tmp = Data[df].groupby(\n",
    "        ['city', 'visit_date'], \n",
    "        as_index=False).res_date_sum.agg({'res_city_date_mean': np.mean})\n",
    "    \n",
    "    tmp['DoW'] = tmp['visit_date'].apply(lambda d: d.weekday())\n",
    "    \n",
    "    # Calculate the mean of total reservations per day of week for each municipality, for both sites\n",
    "    tmp2 = Data[df].groupby(\n",
    "        ['city', 'DoW'], \n",
    "        as_index=False).res_date_sum.agg({'res_city_DoW_mean': np.mean})\n",
    "\n",
    "    tmp = tmp.merge(tmp2,\n",
    "                   how = 'left',\n",
    "                   on = ['city', 'DoW'])\n",
    "    tmp['res_intensity'] = tmp['res_city_date_mean'] / tmp['res_city_DoW_mean']\n",
    " \n",
    "    # Complete the missing reservations with the average on the same city\n",
    "    \n",
    "    missings_train = train['res_intensity_'+site].isnull()\n",
    "    missings_test = test['res_intensity_'+site].isnull()\n",
    "\n",
    "    train.loc[missings_train,['res_intensity_'+ site] ] = train[missings_train][['city','visit_date']].merge(tmp[['city', 'visit_date', 'res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['city','visit_date'])['res_intensity']\n",
    "    \n",
    "    test.loc[missings_test,['res_intensity_'+ site ]] = test[missings_test][['city','visit_date']].merge(tmp[['city', 'visit_date', 'res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['city','visit_date'])['res_intensity']\n",
    "    \n",
    "    # Complete the missing reservations with the average on the whole set\n",
    "    \n",
    "    # Calculate the mean of total reservations per day of week for all restaurants, for both sites\n",
    "    tmp = Data[df].groupby(\n",
    "        ['DoW'], \n",
    "        as_index=False).res_date_sum.agg({'res_all_DoW_mean': np.mean})\n",
    "\n",
    "       \n",
    "    # Calculate the mean of total reservations per date for all restaurants, for both sites\n",
    "    tmp2 = Data[df].groupby(\n",
    "        ['visit_date'], \n",
    "        as_index=False).res_date_sum.agg({'res_all_date_mean': np.mean}) \n",
    "    \n",
    "    tmp = tmp.merge(tmp2,\n",
    "                   how = 'left',\n",
    "                   on = ['DoW'])\n",
    "    \n",
    "    tmp['res_intensity'] = tmp['res_all_date_mean'] / tmp['res_all_DoW_mean']\n",
    "        \n",
    "    missings_train = train['res_intensity_'+site].isnull()\n",
    "    missings_test = test['res_intensity_'+site].isnull()\n",
    "    \n",
    "    train.loc[missings_train,['res_intensity_'+ site] ] = train[missings_train][['city','visit_date']].merge(tmp[['visit_date', 'res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['visit_date'])['res_intensity']\n",
    "    \n",
    "    test.loc[missings_test,['res_intensity_'+ site ]] = test[missings_test][['visit_date']].merge(tmp[['visit_date', 'res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['visit_date'])['res_intensity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>DoW</th>\n",
       "      <th>min_visitors</th>\n",
       "      <th>mean_visitors</th>\n",
       "      <th>median_visitors</th>\n",
       "      <th>max_visitors</th>\n",
       "      <th>count_observations</th>\n",
       "      <th>air_area_name</th>\n",
       "      <th>...</th>\n",
       "      <th>air_genre_name_Japanese food</th>\n",
       "      <th>air_genre_name_Karaoke/Party</th>\n",
       "      <th>air_genre_name_Okonomiyaki/Monja/Teppanyaki</th>\n",
       "      <th>air_genre_name_Other</th>\n",
       "      <th>air_genre_name_Western food</th>\n",
       "      <th>air_genre_name_Yakiniku/Korean food</th>\n",
       "      <th>city</th>\n",
       "      <th>prefecture</th>\n",
       "      <th>res_intensity_air</th>\n",
       "      <th>res_intensity_hpg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>252108</td>\n",
       "      <td>252108</td>\n",
       "      <td>252108.000000</td>\n",
       "      <td>252108.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468</td>\n",
       "      <td>...</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468.000000</td>\n",
       "      <td>250468</td>\n",
       "      <td>250468</td>\n",
       "      <td>209253.000000</td>\n",
       "      <td>226567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>829</td>\n",
       "      <td>478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>air_5c817ef28f236bdf</td>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fukuoka-ken Fukuoka-shi Daimyō</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fukuoka-shi</td>\n",
       "      <td>Tōkyō-to</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>477</td>\n",
       "      <td>799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19637</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31856</td>\n",
       "      <td>132360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.973761</td>\n",
       "      <td>3.019678</td>\n",
       "      <td>5.346455</td>\n",
       "      <td>20.959308</td>\n",
       "      <td>19.822939</td>\n",
       "      <td>49.638042</td>\n",
       "      <td>50.364438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075016</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.014796</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.019551</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946115</td>\n",
       "      <td>0.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.757007</td>\n",
       "      <td>1.923985</td>\n",
       "      <td>6.365183</td>\n",
       "      <td>12.576781</td>\n",
       "      <td>12.715473</td>\n",
       "      <td>36.925611</td>\n",
       "      <td>14.082559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263417</td>\n",
       "      <td>0.045342</td>\n",
       "      <td>0.120737</td>\n",
       "      <td>0.178434</td>\n",
       "      <td>0.138453</td>\n",
       "      <td>0.165109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.414487</td>\n",
       "      <td>0.392044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.125000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726707</td>\n",
       "      <td>0.739937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18.380952</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.877850</td>\n",
       "      <td>0.883788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.077000</td>\n",
       "      <td>1.069724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>877.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>147.428571</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>877.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.807731</td>\n",
       "      <td>10.516129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                air_store_id  visit_date       visitors            DoW  \\\n",
       "count                 252108      252108  252108.000000  252108.000000   \n",
       "unique                   829         478            NaN            NaN   \n",
       "top     air_5c817ef28f236bdf  2017-03-17            NaN            NaN   \n",
       "freq                     477         799            NaN            NaN   \n",
       "mean                     NaN         NaN      20.973761       3.019678   \n",
       "std                      NaN         NaN      16.757007       1.923985   \n",
       "min                      NaN         NaN       1.000000       0.000000   \n",
       "25%                      NaN         NaN       9.000000       1.000000   \n",
       "50%                      NaN         NaN      17.000000       3.000000   \n",
       "75%                      NaN         NaN      29.000000       5.000000   \n",
       "max                      NaN         NaN     877.000000       6.000000   \n",
       "\n",
       "         min_visitors  mean_visitors  median_visitors   max_visitors  \\\n",
       "count   250468.000000  250468.000000    250468.000000  250468.000000   \n",
       "unique            NaN            NaN              NaN            NaN   \n",
       "top               NaN            NaN              NaN            NaN   \n",
       "freq              NaN            NaN              NaN            NaN   \n",
       "mean         5.346455      20.959308        19.822939      49.638042   \n",
       "std          6.365183      12.576781        12.715473      36.925611   \n",
       "min          1.000000       1.000000         1.000000       1.000000   \n",
       "25%          1.000000      11.125000        10.000000      28.000000   \n",
       "50%          3.000000      18.380952        17.000000      43.000000   \n",
       "75%          7.000000      28.125000        27.000000      62.000000   \n",
       "max        132.000000     147.428571       152.000000     877.000000   \n",
       "\n",
       "        count_observations                   air_area_name        ...          \\\n",
       "count        250468.000000                          250468        ...           \n",
       "unique                 NaN                             103        ...           \n",
       "top                    NaN  Fukuoka-ken Fukuoka-shi Daimyō        ...           \n",
       "freq                   NaN                           19637        ...           \n",
       "mean             50.364438                             NaN        ...           \n",
       "std              14.082559                             NaN        ...           \n",
       "min               1.000000                             NaN        ...           \n",
       "25%              40.000000                             NaN        ...           \n",
       "50%              43.000000                             NaN        ...           \n",
       "75%              65.000000                             NaN        ...           \n",
       "max              69.000000                             NaN        ...           \n",
       "\n",
       "        air_genre_name_Japanese food  air_genre_name_Karaoke/Party  \\\n",
       "count                  250468.000000                 250468.000000   \n",
       "unique                           NaN                           NaN   \n",
       "top                              NaN                           NaN   \n",
       "freq                             NaN                           NaN   \n",
       "mean                        0.075016                      0.002060   \n",
       "std                         0.263417                      0.045342   \n",
       "min                         0.000000                      0.000000   \n",
       "25%                         0.000000                      0.000000   \n",
       "50%                         0.000000                      0.000000   \n",
       "75%                         0.000000                      0.000000   \n",
       "max                         1.000000                      1.000000   \n",
       "\n",
       "        air_genre_name_Okonomiyaki/Monja/Teppanyaki  air_genre_name_Other  \\\n",
       "count                                 250468.000000         250468.000000   \n",
       "unique                                          NaN                   NaN   \n",
       "top                                             NaN                   NaN   \n",
       "freq                                            NaN                   NaN   \n",
       "mean                                       0.014796              0.032922   \n",
       "std                                        0.120737              0.178434   \n",
       "min                                        0.000000              0.000000   \n",
       "25%                                        0.000000              0.000000   \n",
       "50%                                        0.000000              0.000000   \n",
       "75%                                        0.000000              0.000000   \n",
       "max                                        1.000000              1.000000   \n",
       "\n",
       "        air_genre_name_Western food  air_genre_name_Yakiniku/Korean food  \\\n",
       "count                 250468.000000                        250468.000000   \n",
       "unique                          NaN                                  NaN   \n",
       "top                             NaN                                  NaN   \n",
       "freq                            NaN                                  NaN   \n",
       "mean                       0.019551                             0.028047   \n",
       "std                        0.138453                             0.165109   \n",
       "min                        0.000000                             0.000000   \n",
       "25%                        0.000000                             0.000000   \n",
       "50%                        0.000000                             0.000000   \n",
       "75%                        0.000000                             0.000000   \n",
       "max                        1.000000                             1.000000   \n",
       "\n",
       "               city  prefecture  res_intensity_air  res_intensity_hpg  \n",
       "count        250468      250468      209253.000000      226567.000000  \n",
       "unique           55           9                NaN                NaN  \n",
       "top     Fukuoka-shi    Tōkyō-to                NaN                NaN  \n",
       "freq          31856      132360                NaN                NaN  \n",
       "mean            NaN         NaN           0.946115           0.948600  \n",
       "std             NaN         NaN           0.414487           0.392044  \n",
       "min             NaN         NaN           0.015771           0.083333  \n",
       "25%             NaN         NaN           0.726707           0.739937  \n",
       "50%             NaN         NaN           0.877850           0.883788  \n",
       "75%             NaN         NaN           1.077000           1.069724  \n",
       "max             NaN         NaN          10.807731          10.516129  \n",
       "\n",
       "[11 rows x 30 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    Data[df] = Data[df].merge(tmp,\n",
    "                          how = 'left',\n",
    "                          on = ['city','visit_date'])\n",
    "    \n",
    "    train = train.merge(tmp,\n",
    "                          how = 'left',\n",
    "                          on = ['city','DoW'])\n",
    "    test = test.merge(tmp,\n",
    "                          how = 'left',\n",
    "                          on = ['city','DoW'])\n",
    "    \n",
    "    # Calculate the mean of total reservations per day of week for all restaurants, for both sites\n",
    "    tmp = Data[df].groupby(\n",
    "        ['DoW'], \n",
    "        as_index=False).res_date_sum.agg({'res_all_DoW_mean': np.mean})\n",
    "\n",
    "    Data[df] = Data[df].merge(tmp,\n",
    "                          how = 'left',\n",
    "                          on = ['DoW'])    \n",
    "    \n",
    "    # Calculate the mean of total reservations per date for all restaurants, for both sites\n",
    "    tmp = Data[df].groupby(\n",
    "        ['visit_date'], \n",
    "        as_index=False).res_date_sum.agg({'res_all_date_mean': np.mean})    \n",
    "\n",
    "    Data[df] = Data[df].merge(tmp,\n",
    "                          how = 'left',\n",
    "                          on = ['visit_date'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air_store_id',\n",
       " 'visit_date',\n",
       " 'visitors',\n",
       " 'DoW',\n",
       " 'min_visitors',\n",
       " 'mean_visitors',\n",
       " 'median_visitors',\n",
       " 'max_visitors',\n",
       " 'count_observations',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'air_genre_name_Asian',\n",
       " 'air_genre_name_Bar/Cocktail',\n",
       " 'air_genre_name_Cafe/Sweets',\n",
       " 'air_genre_name_Creative cuisine',\n",
       " 'air_genre_name_Dining bar',\n",
       " 'air_genre_name_International cuisine',\n",
       " 'air_genre_name_Italian/French',\n",
       " 'air_genre_name_Izakaya',\n",
       " 'air_genre_name_Japanese food',\n",
       " 'air_genre_name_Karaoke/Party',\n",
       " 'air_genre_name_Okonomiyaki/Monja/Teppanyaki',\n",
       " 'air_genre_name_Other',\n",
       " 'air_genre_name_Western food',\n",
       " 'air_genre_name_Yakiniku/Korean food',\n",
       " 'res_intensity_x',\n",
       " 'res_intensity_y']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Air and HPG sites reservations data\n",
    "# from https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "# This method have several drawbacks: we only have data for a few restaurants, what to put for others? \n",
    "# Also the incomplete reservations (afer April 22nd are treated as complete reservations: the submission error is much worse than the evaluation one\n",
    "\n",
    "Data['reserve_air'] = data_load['air_reserve'].copy()\n",
    "Data['reserve_hpg'] = data_load['hpg_reserve'].copy()\n",
    "\n",
    "Data['reserve_hpg'] = pd.merge(Data['reserve_hpg'], data_load['store_id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['reserve_air','reserve_hpg']:\n",
    "    \n",
    "    Data[df]['visit_date'] = Data[df]['visit_datetime'].apply(lambda d: d.date())\n",
    "    # Data[df]['reserve_date'] = Data[df]['reserve_datetime'].dt.date \n",
    "    # Data[df]['reserve_date_diff'] = Data[df].apply(\n",
    "    #    lambda r: (r['visit_date'] - r['reserve_date']).days, axis=1)\n",
    "    \n",
    "    Data[df] = Data[df].groupby(['air_store_id','visit_date'],as_index=False\n",
    "                           ).reserve_visitors.sum().rename(columns={'reserve_visitors':'res_date_sum'})\n",
    "        \n",
    "    # Improve the data, we will compare the reservations with the average reservations on the same day of the week\n",
    "\n",
    "    Data[df]['DoW'] = Data[df]['visit_date'].apply(lambda d: d.weekday())\n",
    "    \n",
    "    # Calculate the mean of total reservations per day of week\n",
    "    # There was an error here: sum must be made on the total of reservations, not on each reservation line \n",
    "    tmp0 = Data[df].groupby(\n",
    "        ['air_store_id', 'DoW'], \n",
    "        as_index=False).res_date_sum.agg({'res_DoW_mean': np.mean})\n",
    "    \n",
    "    Data[df] = Data[df].merge(tmp0, how='inner', on=['air_store_id','DoW'])\n",
    "    \n",
    "    Data[df]['res_intensity'] = Data[df].res_date_sum / Data[df].res_DoW_mean \n",
    "    \n",
    "    train = pd.merge(train, Data[df][['air_store_id','visit_date','res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['air_store_id','visit_date']).fillna(1)\n",
    "    \n",
    "    test = pd.merge(test, Data[df][['air_store_id','visit_date','res_intensity']], how='left', on=['air_store_id','visit_date']).fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "DATA_LEARNING_FILE = \"../../data/Data_visit-20171223-2\"\n",
    "DATA_SUBMISSSION_FILE = \"../../data/Data_sub-20171223-2\"\n",
    "\n",
    "train.to_pickle(DATA_LEARNING_FILE)\n",
    "test.to_pickle(DATA_SUBMISSSION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data[df].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Calculate the total of reservations per day for restaurants with reservations in both sites\n",
    "    tmp_store_date_sum = Data[df].groupby(['air_store_id','visit_date'],as_index=False\n",
    "                           ).reserve_visitors.sum().rename(columns={'reserve_visitors':'store_date_sum'})\n",
    "    tmp_store_date_sum['DoW'] = tmp_store_date_sum.visit_date.apply(lambda d: d.weekday() )\n",
    "    \n",
    "    Data[df] = pd.merge(tmp_store_date_sum, tmp_store_DoW, how='inner', on=['air_store_id','DoW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    Data[df]['visit_date'] = Data[df]['visit_datetime'].dt.date\n",
    "    Data[df]['reserve_date'] = Data[df]['reserve_datetime'].dt.date \n",
    "    Data[df]['reserve_date_diff'] = Data[df].apply(\n",
    "        lambda r: (r['visit_date'] - r['reserve_date']).days, axis=1)\n",
    "    \n",
    "    # Improve the data, we will compare the reservations with the average reservations on the same day of the week\n",
    "    Data[df]['DoW'] = Data[df]['visit_datetime'].dt.dayofweek\n",
    "    \n",
    "    # Calculate the mean of total reservations per day of week\n",
    "    tmp0 = Data[df].groupby(\n",
    "        ['air_store_id', 'DoW'], \n",
    "        as_index=False).reserve_visitors.agg({'res_DoW_mean': np.mean})\n",
    "    \n",
    "    \n",
    "    tmp1 = Data[df].groupby(['air_store_id','visit_date'],as_index=False\n",
    "                           ).reserve_visitors.sum().rename(columns={'reserve_visitors':'res_date_sum'})\n",
    "    tmp1['DoW'] = tmp1.visit_date.apply(lambda d: d.weekday() )\n",
    "    \n",
    "    \n",
    "    Data[df] = pd.merge(tmp1, tmp0, how='inner', on=['air_store_id','DoW'])\n",
    "    \n",
    "    Data[df]['res_intensity'] = Data[df].res_date_sum / Data[df].res_DoW_mean \n",
    "    \n",
    "    # By doing so, we replace air reservations data by HPG reservations data for restaurants in both sites!!\n",
    "    train = pd.merge(train, Data[df][['air_store_id','visit_date','res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['air_store_id','visit_date']).fillna(1)\n",
    "    test = pd.merge(test, Data[df][['air_store_id','visit_date','res_intensity']], \n",
    "                    how='left', \n",
    "                    on=['air_store_id','visit_date']).fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data files\n",
    "* Data_visit-20171222-1: without area name 1H\n",
    "* Data_visit-20171222-2: without area name 1H, removing week end holiday => no effect\n",
    "* Data_visit-20171222-3: idem, with month as cos, sin => no effect\n",
    "* Data_visit-20171222-4: idem with delta_days => no effect\n",
    "* Data_visit-20171222-5: idem with reservations, minimal method\n",
    "* Data_visit-20171223-1: idem with reservations, using intensity (1 for missing values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data files:\n",
    "* Data_visit-20171217-1: data with overall reservations and holyday eve flag features\n",
    "* Data_visit-20171219-1: data with overall reservations pe city and holiday eve flag features (only 779 stores)\n",
    "* Data_visit-20171219-3: data with overall reservations pe city and holiday eve flag features (only 779 stores) restaurant visits statistics are calculated on 2017 only (was also on 2016 before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the mean number of reservations for each visit_date \n",
    "print(\"Merging for total\")\n",
    "\n",
    "Data_reserve_hpg = Data_reserve_hpg.merge(\n",
    "    data_load['hpg_store'],\n",
    "    how = 'inner',\n",
    "    on = 'hpg_store_id')\n",
    "\n",
    "Data_reserve_hpg['city'] = Data_reserve_hpg['hpg_area_name'].apply(lambda x: x.split(' ')[1]) \n",
    "\n",
    "# Get the total of reserve seats per date, hpg area, restaurants\n",
    "A = Data_reserve_hpg.groupby(['visit_date', 'city', 'hpg_store_id'], as_index=False).agg({'reserve_visitors': np.sum})\n",
    "\n",
    "# Get the reservations statistics per date, hpg area\n",
    "A = A.groupby(['visit_date', 'city' ], as_index=False).reserve_visitors.agg({\n",
    "                                    'reserve_visitors_mean': np.mean,\n",
    "                                    'reserve_visitors_median': np.median,\n",
    "                                    'reserve_visitors_count': np.count_nonzero,\n",
    "                                    'reserve_visitors_max': np.max,\n",
    "                                    'reserve_visitors_min': np.min\n",
    "                                  })\n",
    "\n",
    "Data_visit['city'] = Data_visit['air_area_name'].apply(lambda x: x.split(' ')[1]) \n",
    "\n",
    "Data_visit = Data_visit.merge(\n",
    "                            A,\n",
    "                            how='inner',\n",
    "                            on = ['visit_date','city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Air and HPG sites reservations data\n",
    "# from https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "# This method have several drawbacks: we only have data for a few restaurants, what to put for others? \n",
    "# Also the incomplete reservations (afer April 22nd are treated as complete reservations: the submission error is much worse than the evaluation one\n",
    "\n",
    "Data['reserve_air'] = data_load['air_reserve'].copy()\n",
    "Data['reserve_hpg'] = data_load['hpg_reserve'].copy()\n",
    "\n",
    "Data['reserve_hpg'] = pd.merge(Data['reserve_hpg'], data_load['store_id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['reserve_air','reserve_hpg']:\n",
    "    \n",
    "    Data[df]['visit_date'] = Data[df]['visit_datetime'].dt.date\n",
    "    Data[df]['reserve_date'] = Data[df]['reserve_datetime'].dt.date \n",
    "    Data[df]['reserve_date_diff'] = Data[df].apply(\n",
    "        lambda r: (r['visit_date'] - r['reserve_date']).days, axis=1)\n",
    "    \n",
    "    # Improve the data, we will compare the reservations with the average reservations on the same day of the week\n",
    "\n",
    "    Data[df]['DoW'] = Data[df]['visit_datetime'].dt.dayofweek\n",
    "    \n",
    "    # Calculate the mean of total reservations per day of week\n",
    "    # There was an error here: sum must be made on the total of reservations, not on each reservation line \n",
    "    tmp0 = Data[df].groupby(\n",
    "        ['air_store_id', 'DoW'], \n",
    "        as_index=False).reserve_visitors.agg({'res_DoW_mean': np.mean})\n",
    "    \n",
    "    \n",
    "    tmp1 = Data[df].groupby(['air_store_id','visit_date'],as_index=False\n",
    "                           ).reserve_visitors.sum().rename(columns={'reserve_visitors':'res_date_sum'})\n",
    "    tmp1['DoW'] = tmp1.visit_date.apply(lambda d: d.weekday() )\n",
    "    \n",
    "    \n",
    "    Data[df] = pd.merge(tmp1, tmp0, how='inner', on=['air_store_id','DoW'])\n",
    "    \n",
    "    Data[df]['res_intensity'] = Data[df].res_date_sum / Data[df].res_DoW_mean \n",
    "    \n",
    "    # By doing so, we replace air reservations data by HPG reservations data for restaurants in both sites!!\n",
    "    train = pd.merge(train, Data[df][['air_store_id','visit_date','res_intensity']], \n",
    "                     how='left', \n",
    "                     on=['air_store_id','visit_date']).fillna(1)\n",
    "    test = pd.merge(test, Data[df][['air_store_id','visit_date','res_intensity']], \n",
    "                    how='left', \n",
    "                    on=['air_store_id','visit_date']).fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To include reservations - old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Air and HPG sites reservations data\n",
    "# from https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "# This method have several drawbacks: we only have data for a few restaurants, what to put for others? \n",
    "# Also the incomplete reservations (afer April 22nd are treated as complete reservations: the submission error is much worse than the evaluation one\n",
    "\n",
    "Data['reserve_air'] = data_load['air_reserve'].copy()\n",
    "Data['reserve_hpg'] = data_load['hpg_reserve'].copy()\n",
    "\n",
    "Data['reserve_hpg'] = pd.merge(Data['reserve_hpg'], data_load['store_id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['reserve_air','reserve_hpg']:\n",
    "    Data[df]['visit_date'] = Data[df]['visit_datetime'].dt.date\n",
    "    Data[df]['reserve_date'] = Data[df]['reserve_datetime'].dt.date \n",
    "    Data[df]['reserve_date_diff'] = Data[df].apply(\n",
    "        lambda r: (r['visit_date'] - r['reserve_date']).days, axis=1)\n",
    "    \n",
    "    tmp1 = Data[df].groupby(['air_store_id','visit_date'],as_index=False\n",
    "                           ).reserve_visitors.sum().rename(columns={'reserve_visitors':'rv_sum'})\n",
    "    tmp2 = Data[df].groupby(['air_store_id','visit_date'], as_index=False\n",
    "                           )[['reserve_date_diff','reserve_visitors']].mean().rename(columns={ 'reserve_date_diff': 'rdd_mean', \n",
    "                                                                                 'reserve_visitors':'rv_mean'})\n",
    "    Data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])   \n",
    "    \n",
    "    # By doing so, we replace air reservations data by HPG reservations data for restaurants in both sites!!\n",
    "    train = pd.merge(train, Data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, Data[df], how='left', on=['air_store_id','visit_date'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
