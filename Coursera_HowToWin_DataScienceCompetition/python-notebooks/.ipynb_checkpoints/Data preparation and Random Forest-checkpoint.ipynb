{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "* Step 1: prepare the training and evaluation data set\n",
    "* Step 2: training with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_extraction\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "# To use part or all the train set\n",
    "training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load all Files (hey must be in input directory in a brother directory of the notebook)\n",
    "data_load = {\n",
    "    'item_categories': pd.read_csv('../input/item_categories.csv'), \n",
    "    'items': pd.read_csv('../input/items.csv'), \n",
    "    'sales_train': pd.read_csv('../input/sales_train_v2.csv'),\n",
    "    'sample_submission': pd.read_csv('../input/sample_submission.csv'),\n",
    "    'shops': pd.read_csv('../input/shops.csv'),\n",
    "    'test': pd.read_csv('../input/test.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data = {}\n",
    "\n",
    "# Sales data \n",
    "data_load['sales_train']['date'] = pd.to_datetime(data_load['sales_train']['date'], format = \"%d.%m.%Y\")\n",
    "\n",
    "#Data['sales']['day'] = transactions['date'].dt.day\n",
    "data_load['sales_train']['month'] = data_load['sales_train']['date'].dt.month\n",
    "data_load['sales_train']['year'] = data_load['sales_train']['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load['sales_train'][(data_load['sales_train'].shop_id == 5) & (data_load['sales_train'].item_id == 485)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load['sales_train'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training/evaluation set, with similar pattern as the test set:\n",
    "* All shops in both sets\n",
    "* evaluation set on the last month\n",
    "* unknown items in the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Calculate the total of items sold per month, item, shop\n",
    "\n",
    "Data['train'] = data_load['sales_train'].groupby(['date_block_num', 'shop_id', 'item_id'], as_index = False).item_cnt_day.agg({\n",
    "    'item_cnt_month': np.sum\n",
    "})\n",
    "\n",
    "# We calculate the average per line, not weighted by the number of sales to simplify (and avoid division per 0)\n",
    "\n",
    "tmp = data_load['sales_train'].groupby(['date_block_num', 'shop_id', 'item_id'], as_index= False).item_price.agg({\n",
    "    'item_mean_price_shop_month': np.mean\n",
    "})\n",
    "\n",
    "Data['train'] = Data['train'].merge(tmp,\n",
    "                    how = 'left',\n",
    "                    on = ['date_block_num', 'shop_id', 'item_id'])\n",
    "\n",
    "if training: \n",
    "    \n",
    "    # Split on date\n",
    "    \n",
    "    condition = Data['train']['date_block_num']==33\n",
    "    Data['evaluation'] = Data['train'][condition]\n",
    "    Data['train'] = Data['train'][~condition]\n",
    "    \n",
    "    print(\"sizes:\" ,Data['evaluation'].shape, Data['train'].shape)\n",
    "\n",
    "    # The following part was disable, because the evaluation set already presents a larger proportion of unknown items than \n",
    "    # the test set\n",
    "    \n",
    "    # Select 7.1% of the items to remove from the data set (363 on 5100 )\n",
    "    # This may be too high, as some items could be appearing only in this last month: TO CHECK > in fact the proportion \n",
    "    # of missing item is already too large\n",
    "\n",
    "    #list_item_ids = list(Data['train'].item_id.unique())\n",
    "    #n = int(363/5100*21807)\n",
    "\n",
    "    # Remove the items from training set\n",
    "\n",
    "    #removed_item_ids = random.sample(list_item_ids, n)\n",
    "    #print(\"Number of items removed from train set:\", len(removed_item_ids))\n",
    "\n",
    "    #condition = Data['train'].item_id.isin(removed_item_ids)\n",
    "    #Data['train'] = Data['train'][~condition]\n",
    "\n",
    "    #print(Data['evaluation'].shape, Data['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Data['train'][( Data['train'].shop_id == 5) & ( Data['train'].item_id == 485)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winsorization - on training set only to evaluate the effect on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_limit = Data['train'].item_mean_price_shop_month.quantile([0.0, 0.999])[0.999]\n",
    "prices = Data['train'].item_mean_price_shop_month\n",
    "Data['train'].loc[(prices > price_limit), 'item_mean_price_shop_month'] = price_limit\n",
    "\n",
    "# clipping must be done after the stats are calculated\n",
    "#Data['train'].loc[:,'item_cnt_month'] = Data['train'].item_cnt_month.clip(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    set_list = ['train', 'evaluation']\n",
    "else:\n",
    "    set_list = ['train']\n",
    "    \n",
    "months = data_load['sales_train'].groupby(['month', 'year'], as_index = False).date_block_num.first()\n",
    "\n",
    "for set_name in set_list:\n",
    "\n",
    "    # Add the missing rows date_block_num,shop_id, item_id with item_cnt_month = 0\n",
    "    \n",
    "    print('Adding the missing rows for', set_name, 'set...')\n",
    "    unique_date_block_num = sorted(Data[set_name].date_block_num.unique())\n",
    "    unique_shop_id = sorted(Data[set_name].shop_id.unique())\n",
    "    unique_item_id = sorted(Data[set_name].item_id.unique())\n",
    "\n",
    "    d = {\n",
    "        'date_block_num': unique_date_block_num,\n",
    "        'item_id': unique_item_id,\n",
    "        'shop_id': unique_shop_id\n",
    "    }\n",
    "\n",
    "    tmp = list(itertools.product(*[unique_date_block_num, unique_item_id, unique_shop_id]))\n",
    "    od = OrderedDict(sorted(d.items()))\n",
    "\n",
    "    df = pd.DataFrame(tmp,columns=od.keys())\n",
    "    Data[set_name] = df.merge(Data[set_name],\n",
    "                             how= 'left',\n",
    "                             on = ['shop_id', 'item_id', 'date_block_num'])\n",
    "\n",
    "    # Add the month and year\n",
    "    \n",
    "    Data[set_name] = Data[set_name].merge(months,\n",
    "                   how = 'left',\n",
    "                   on = ['date_block_num'])\n",
    "    \n",
    "    # For new rows missing values:\n",
    "    \n",
    "    Data[set_name].item_cnt_month = Data[set_name].item_cnt_month.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['train'][ (Data['train'].date_block_num == 6) & ( Data['train'].shop_id == 5) & ( Data['train'].item_id == 485)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Add the lagged information on item count\n",
    "print(\"Adding item count lagged values\")\n",
    "\n",
    "Data['train']['item_cnt_month_minus_12'] = 0\n",
    "Data['train']['item_cnt_month_minus_3'] = 0\n",
    "Data['train']['item_cnt_month_minus_1'] = 0 \n",
    "\n",
    "for it_month in range(12, 33,1):\n",
    "    condition = Data['train'].date_block_num == it_month\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_12'] = Data['train'][(Data['train'].date_block_num == it_month-12)].as_matrix(columns=['item_cnt_month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['train'][ (Data['train'].date_block_num == 18) & ( Data['train'].shop_id == 5) & ( Data['train'].item_id == 485)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'train' must be first in those lists\n",
    "if training:\n",
    "    set_list = ['train', 'test' ,'evaluation']\n",
    "else:\n",
    "    set_list = ['train', 'test']\n",
    "\n",
    "# Prepare the test set\n",
    "print(\"Preparing the test set\")\n",
    "\n",
    "Data['test'] = data_load['test'].copy()\n",
    "Data['test']['month'] = 11\n",
    "Data['test']['year'] = 2015\n",
    "Data['test']['date_block_num'] = 34\n",
    "\n",
    "cols = ['ID', 'date_block_num', 'item_id', 'shop_id','month', 'year']\n",
    "Data['test'] = Data['test'][cols]\n",
    "\n",
    "#print(Data['test'].columns.tolist())\n",
    "\n",
    "### Add the item_category\n",
    "\n",
    "print(\"Adding item category stats\")\n",
    "\n",
    "for set_name in set_list:\n",
    "    Data[set_name] = Data[set_name].merge(data_load['items'],\n",
    "            how = 'left',\n",
    "            on = 'item_id',\n",
    "            ).drop(['item_name'], axis = 1)\n",
    "\n",
    "# Stats over the items and the months\n",
    "tmp = Data['train'].groupby(['shop_id', 'item_category_id'], as_index = False).item_cnt_month.agg({\n",
    "    'category_cnt_month_mean': np.mean,   \n",
    "    'category_cnt_month_max': np.max\n",
    "})\n",
    "\n",
    "for set_name in set_list:\n",
    "    Data[set_name] = Data[set_name].merge(tmp,\n",
    "            how = 'left',\n",
    "            on = ['item_category_id', 'shop_id']\n",
    "            )\n",
    "    \n",
    "# The category without sales for this restaurant will remain with a NaN (removed when creating the X/y)\n",
    "\n",
    "### Add the lagged information on item count\n",
    "print(\"Adding item count lagged values\")\n",
    "\n",
    "Data['train']['item_cnt_month_minus_12'] = 0\n",
    "Data['train']['item_cnt_month_minus_3'] = 0\n",
    "Data['train']['item_cnt_month_minus_1'] = 0 \n",
    "\n",
    "for it_month in range(12, 33,1):\n",
    "    condition = Data['train'].date_block_num == it_month\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_12'] = Data['train'][(Data['train'].date_block_num == it_month-12)].as_matrix(columns=['item_cnt_month'])\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_3'] = Data['train'][(Data['train'].date_block_num == it_month-3)].as_matrix(columns=['item_cnt_month'])\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_1'] = Data['train'][(Data['train'].date_block_num == it_month-1)].as_matrix(columns=['item_cnt_month'])\n",
    "\n",
    "if training:\n",
    "    for lag in [1,3,12]:\n",
    "        print(lag)\n",
    "        tmp = Data['train'][Data['train'].date_block_num == (33-lag)].item_cnt_month\n",
    "        Data['evaluation'] = Data['evaluation'].merge(tmp,\n",
    "                how = 'left',\n",
    "                on = ['item_id', 'shop_id']\n",
    "                ).rename(columns={'item_cnt_month':'item_cnt_month_minus_'+lag })    \n",
    "else:     \n",
    "    for lag in [1,3,12]:\n",
    "        print(lag)\n",
    "        tmp = Data['train'][Data['train'].date_block_num == (34-lag)][['item_cnt_month']]\n",
    "        Data['test'] = Data['test'].merge(tmp,\n",
    "                how = 'left',\n",
    "                on = ['item_id', 'shop_id']\n",
    "                ).rename(columns={'item_cnt_month':'item_cnt_month_minus_'+lag })\n",
    "\n",
    "  \n",
    "    \n",
    "### Add the stats on item_cnt_month for every couple store,item\n",
    "# Only known values are for the train set\n",
    "\n",
    "print(\"Adding item count stats\")\n",
    "\n",
    "# Stats over the months\n",
    "tmp = Data['train'].groupby(['shop_id', 'item_id'], as_index = False).item_cnt_month.agg({\n",
    "    'item_cnt_month_mean': np.mean,\n",
    "    'item_cnt_month_max': np.max\n",
    "})\n",
    "# Warning: if the couple item, shop is not present in the train set for any month, the following will generate a NaN:\n",
    "# * for merge with the train set: this will produce a 0 (all couples exist) > we remove those later as they are all the same\n",
    "# and cannot be used to learn anything\n",
    "# * for merge with evaluation/test set: this will produce a NaN > those lines will be replaced by stats\n",
    "# > this is wrong, the system cannot learn, we have to replace those lines in the eval/test after prediction\n",
    "\n",
    "for set_name in set_list:\n",
    "    Data[set_name] = Data[set_name].merge(tmp,\n",
    "            how = 'left',\n",
    "            on = ['item_id', 'shop_id']\n",
    "            )\n",
    "    \n",
    "# Add the average price per item and shop, over the train data set (average price will be missing for some items)\n",
    "\n",
    "print(\"Adding shop item price stats\")\n",
    "\n",
    "# Average over the months of the train set per item and shop\n",
    "# The mean is not weighted by the sales\n",
    "\n",
    "tmp = Data['train'].groupby(['item_id', 'shop_id'], as_index = False).item_mean_price_shop_month.agg({\n",
    "    'item_shop_mean_price': np.mean\n",
    "})\n",
    "\n",
    "for set_name in set_list:\n",
    "    Data[set_name] = Data[set_name].merge(tmp,\n",
    "            how = 'left',\n",
    "            on = ['item_id', 'shop_id']\n",
    "            )\n",
    "\n",
    "# We remove the couple (item, shop) without any sale from the training set\n",
    "condition = pd.isnull(Data['train'].item_shop_mean_price)\n",
    "Data['train'] = Data['train'][~condition]\n",
    "    \n",
    "    #missing_shop_item_indices = pd.isnull(Data[set_name]['item_shop_mean_price'])\n",
    "    #Data[set_name].loc[missing_shop_item_indices, 'item_shop_mean_price'] = -999\n",
    "\n",
    "print(\"Adding overall item price stats\")    \n",
    "    \n",
    "# Add the average price per item over the train data set (average price will be missing for some items)\n",
    "# The mean is not weighted by the sales\n",
    "\n",
    "tmp = Data['train'].groupby(['item_id'], as_index = False).item_mean_price_shop_month.agg({\n",
    "    'item_overall_mean_price': np.mean\n",
    "})\n",
    "\n",
    "for set_name in set_list:\n",
    "    Data[set_name] = Data[set_name].merge(tmp,\n",
    "            how = 'left',\n",
    "            on = ['item_id']\n",
    "            )\n",
    "    # The item with no sale in the training set will be addressed after prediction\n",
    "    # missing_item_indices = pd.isnull(Data[set_name]['item_overall_mean_price'])\n",
    "    # Data[set_name].loc[missing_item_indices, 'item_overall_mean_price'] = -999\n",
    "\n",
    "# Remove the item_id TO PUT BACK\n",
    "\n",
    "#for set_name in set_list:\n",
    "#    Data[set_name] = Data[set_name].drop(['item_id', 'item_category_id'], axis = 1)\n",
    "    \n",
    "Data['train'] = Data['train'].drop(['item_mean_price_shop_month'], axis = 1)\n",
    "if training:\n",
    "    Data['evaluation'] = Data['evaluation'].drop(['item_mean_price_shop_month'], axis = 1)\n",
    "\n",
    "print(\"Done!\")\n",
    "#Data['evaluation'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Data['train'][Data['train'].date_block_num == (34-lag)][['item_cnt_month']]\n",
    "print(lag)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    print(Data['evaluation'].shape, Data['train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "DATA_LEARNING_FILE = \"../data/sales-20180128g\"\n",
    "DATA_EVALUATION_FILE = \"../data/evaluation-20180128g\"\n",
    "DATA_TEST_FILE = \"../data/test-20180128g\"\n",
    "\n",
    "Data['train'].to_pickle(DATA_LEARNING_FILE)\n",
    "if training:\n",
    "    Data['evaluation'].to_pickle(DATA_EVALUATION_FILE)\n",
    "Data['test'].to_pickle(DATA_TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 20180127c : with windsorization 0.999 , removing rows for store, item without any sale\n",
    "# 20180127d : idem without windsorization \n",
    "# 20180127e : idem with windsorization 0.9999\n",
    "# 20180127f : idem with windsorization 0.99\n",
    "# 20180127g : idem with windsorization 0.99, with full set\n",
    "# 20180127h : idem with windsorization 0.999, with full set\n",
    "# 20180128c : windsorization on train only 0.999, with train/eval split\n",
    "# 20180128d : windsorization on train only 0.99, with train/eval split\n",
    "# 20180128e : no windsorization, with train/eval split\n",
    "# 20180128f : with windsor 0.999 on price, with train/eval split\n",
    "# 20180128g : with windsor 0.999 on price, without eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart it to retrieve data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "training = True\n",
    "\n",
    "DATA_LEARNING_FILE = \"../data/sales-20180128f\"\n",
    "DATA_EVALUATION_FILE = \"../data/evaluation-20180128f\"\n",
    "\n",
    "Data = {}\n",
    "\n",
    "Data['train'] = pd.read_pickle(DATA_LEARNING_FILE)\n",
    "if training:\n",
    "    Data['evaluation'] = pd.read_pickle(DATA_EVALUATION_FILE)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['train'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipping  = True\n",
    "\n",
    "# Random split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#train_set, test_set = train_test_split(Data['train'], test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Remove the first year to decrease training time (and after we will add delayed values)\n",
    "Data['train'] = Data['train'][Data['train'].date_block_num >11].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x_train = Data['train'].drop(['item_cnt_month'], axis = 1)\n",
    "\n",
    "if clipping:\n",
    "    y_train = Data['train'].item_cnt_month.clip(0,20)\n",
    "else:\n",
    "    y_train = Data['train'].item_cnt_month\n",
    "\n",
    "# I should remove the evaluation prediction rows with missings category\n",
    "\n",
    "if training:\n",
    "    x_eval = Data['evaluation'].drop(['item_cnt_month'], axis = 1)\n",
    "    y_eval = Data['evaluation'].item_cnt_month\n",
    "\n",
    "del(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def model_evaluation(model_reg, x_train, y_train, x_test, y_test, use_average = True): \n",
    "    sales_predictions = model_reg.predict(x_train)\n",
    "    mse = mean_squared_error(y_train, sales_predictions)\n",
    "    rmse_train = np.sqrt(mse)\n",
    "\n",
    "    sales_predictions = pd.DataFrame({'pred': model_reg.predict(x_test.fillna(0))})\n",
    "    \n",
    "    if use_average:\n",
    "        # replace the shop, item rows with no values, with the average on the category \n",
    "        missing_shop_item_rows = pd.isnull(x_test.item_shop_mean_price)\n",
    "        print('Missing lines for shop,items: ', len(x_test[missing_shop_item_rows]))\n",
    "        sales_predictions.loc[missing_shop_item_rows, 'pred'] = x_test[missing_shop_item_rows].category_cnt_month_mean\n",
    "    \n",
    "    # replace the shop, category with no values, with 0 (the shop is not selling this category)\n",
    "    missing_shop_category_rows = pd.isnull(sales_predictions.pred)  \n",
    "    print('Missing lines for shop,category: ', len(x_test[missing_shop_category_rows]))\n",
    "    sales_predictions.loc[missing_shop_category_rows, 'pred'] = 0\n",
    "    \n",
    "    mse = mean_squared_error(y_test.clip(0,20), sales_predictions.pred.clip(0,20))\n",
    "    rmse_test = np.sqrt(mse)\n",
    "\n",
    "    print(\"train error: \", '{0:.3f}'.format(rmse_train), \"evaluation error: \", '{0:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df_reg = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
    "           max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
    "           n_estimators=100, n_jobs=5, oob_score=False, random_state=None,\n",
    "           verbose=0, warm_start=False)\n",
    "df_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with 20180127c\n",
    "# max_depth=15, n_estimators=20, max_features: 3 : 1.004 / 1.403\n",
    "# max_depth=20, n_estimators=20, max_features: 2 : 0.839 / 1.399\n",
    "# max_depth=20, n_estimators=20, max_features: 3 : 0.818 / 1.394\n",
    "# max_depth=20, n_estimators=50, max_features: 3 : 0.834 / 1.391 > best\n",
    "# max_depth=25, n_estimators=20, max_features: 3 : 0.657 / 1.404\n",
    "# max_depth=20, n_estimators=20, max_features: 4 : 0.795 / 1.410\n",
    "\n",
    "# with 20180127d, max_depth=20, n_estimators=50, max_features: 3\n",
    "# train error: 1.104 evaluation error:  4.596 > windsorization is essential!!\n",
    "\n",
    "# with 20180127e, max_depth=20, n_estimators=25, max_features: 3\n",
    "# train error:  0.949 evaluation error:  1.612 still not as good\n",
    "\n",
    "# with 20180127f, max_depth=20, n_estimators=25, max_features: 3\n",
    "# train error:  0.696 evaluation error:  1.027 > best but beware, \n",
    "#I may just be reducing the variance of the evaluation set :)  \n",
    "# let check with the Kaggle :) \n",
    "# with clip 0-20 train error:  0.692 evaluation error:  0.924 (meaningless as it was windsored)\n",
    "\n",
    "# with 20180127h, max_depth=20, n_estimators=50, max_features: 3, windsor:0.999, full set\n",
    "# kaggle: 1.41\n",
    "\n",
    "# with 20180127g, max_depth=20, n_estimators=50, max_features: 3, windsor:0.99, full set\n",
    "# kaggle: 1.40 > best\n",
    "\n",
    "\n",
    "# Conclusion: we have brought the results much closer 0.7 / 1.0 / 1.4\n",
    "# but we are still higher than before ??? \n",
    "# does the category do help or harm?\n",
    "\n",
    "# with 20180127f, removing category stat, max_depth=20, n_estimators=25, max_features: 3, windsor:0.99, full set\n",
    "# 0.677 evaluation error:  1.043 : slightly worse\n",
    "\n",
    "# with 20180128d, max_depth=20, n_estimators=25, max_features: 3, windsor on train 0.99\n",
    "# train error:  0.601 evaluation error:  5.237\n",
    "# with clipping 0-20: train error:  0.607 evaluation error:  0.979\n",
    "\n",
    "# with 20180128c, max_depth=20, n_estimators=25, max_features: 3, windsor on train 0.999\n",
    "# train error:  0.743 evaluation error:  5.039\n",
    "# with clipping 0-20: train error:  0.743 evaluation error:  0.996\n",
    "\n",
    "# with 20180128e, max_depth=20, n_estimators=25, max_features: 3, no windsor on train\n",
    "# train error:  1.095 evaluation error:  4.702\n",
    "# with clipping 0-20 at evaluation (y_eval and pred): train error:  1.094 evaluation error:  1.002\n",
    "# with clipping 0-20 of y_train train error:  0.616 evaluation error:  0.975 > best\n",
    "\n",
    "# with 20180128f, max_depth=20, n_estimators=25, max_features: 3,  windsor 0.999 on price only, clipping 0-20 on train\n",
    "# train error:  0.616 evaluation error:  0.973 > best\n",
    "\n",
    "# with 20180128g, max_depth=20, n_estimators=100, max_features: 3,  windsor 0.999 on price only, clipping 0-20 on train\n",
    "# no eval\n",
    "# kaggle: 1.02119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(df_reg, '../models/randomforest_20180128g.pkl')\n",
    "#df_reg = joblib.load('../models/randomforest_20180127g.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = df_reg.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# You can experiment with many other options here, using the same .fit() and .predict()\n",
    "# methods; see http://scikit-learn.org\n",
    "# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\n",
    "gbm = xgb.XGBRegressor(max_depth=3, \n",
    "                       n_estimators=300, \n",
    "                       learning_rate=0.05,\n",
    "                       n_jobs = 5).fit(x_train, y_train)\n",
    "\n",
    "if training:\n",
    "    model_evaluation(gbm, x_train, y_train, x_eval, y_eval, use_average = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [10,50, 100], 'max_features': [2,4,6], 'max_depth': [5, 10, 20]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, n_jobs = 4,\n",
    "                          scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_evaluation(grid_search.best_estimator_, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt_reg = GradientBoostingRegressor(learning_rate = 0.1)\n",
    "gbrt_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "x_test =  test_set.drop(['item_cnt_month'], axis = 1)\n",
    "y_test = test_set.item_cnt_month\n",
    "\n",
    "model_evaluation(gbrt_reg, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 20180120-01 with default randomForestRegressor, only 4 features, eval: 4.90 (not better through grid search)\n",
    "* idem, but adding the item_category, eval: 5.4 vs 2.42 (train set)\n",
    "* with GBRT, eval = 7.1 vs 7.36(train set) > not working\n",
    "* with Random Forest, adding the min, max, mean of sales per store,item eval 4.86 vs 2.04\n",
    "* idem, removing the item_id: train error:  2.16 test_error:  4.64\n",
    "* idem, selecting the best estimator: train error:  1.93 test_error:  4.56\n",
    "* adding the average price per item, per shop and overall, max_features 3, n_estimators=50 train error:  train error:  1.98 test_error:  4.45 (best)\n",
    "* with one hot encoding of the shop_id: train error:  1.97 test_error:  4.90\n",
    "* with one hot encoding of the item_category_id: train error:  2.07 test_error:  4.79\n",
    "* with windsorization of the count_item_day, and the price the results improve a lot: train error:  1.08 test_error:  2.91\n",
    "* adding the 0 values: train error:  0.33 test_error:  0.94 > kaggle 1.27 without putting missing items to 0 this does not change a thing actually)\n",
    "* item category is useless > split and provide some price per category\n",
    "* with new train/test set, not removing the new item prediction: train error:  0.34 test_error:  1.94 / Kaggle: 2.61 (I did not remove the 0 prediction there, nor retrain on the full set)\n",
    "* with category stats randomforest_20180126b: tain_error:  0.35 test_error:  1.75 Kaggle: 2.4 \n",
    "* idem, removing missing items: Kaggle: 2.42 it is worse, removing missing category is better!\n",
    "* with full training: Kaggle: 2.58!!! maybe I am just tragically overfitting\n",
    "* trying to reduce overfitting by putting max_features = 2/ min_samples_leaf=2: train error:  0.657 test_error:  1.805\n",
    "* removing the mean, removing the artificial item deletion from validation set: train error:  0.339 evaluation error:  1.768 Kaggle: \n",
    "\n",
    "* with removal of missing shop, item rows, and replacement by avrage and 0, 100 estimators: train error:  0.400 evaluation error:  1.417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Submission preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "DATA_TEST_FILE = \"../data/test-20180128g\"\n",
    "\n",
    "Data = {}\n",
    "\n",
    "Data['test'] = pd.read_pickle(DATA_TEST_FILE)\n",
    "\n",
    "Data['test'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_average = True\n",
    "\n",
    "X_test = Data['test'].drop(['ID'], axis = 1)\n",
    "\n",
    "# Option 1\n",
    "# It trust the model will learn from the category count for the missing item - \n",
    "# for the missing category, I set the prediction to 0\n",
    "# missing_shop_item_indices = pd.isnull(Data['test']['item_cnt_month_mean'])\n",
    "# This is not improving the score!\n",
    "\n",
    "predictions = pd.DataFrame({'pred': df_reg.predict(X_test.fillna(0)).clip(0,20)})\n",
    "\n",
    "# replace the shop, item rows with no values, with the average on the category\n",
    "\n",
    "if use_average:\n",
    "        # replace the shop, item rows with no values, with the average on the category \n",
    "        missing_shop_item_rows = pd.isnull(X_test.item_shop_mean_price)\n",
    "        print('Missing lines for shop,items: ', len(X_test[missing_shop_item_rows]))\n",
    "        predictions.loc[missing_shop_item_rows, 'pred'] = X_test[missing_shop_item_rows].category_cnt_month_mean\n",
    "\n",
    "# for the shop with not category, replace with \n",
    "\n",
    "missing_shop_category_rows = pd.isnull(predictions.pred)\n",
    "print('Missing lines for shop,category: ', len(X_test[missing_shop_category_rows]))\n",
    "predictions.loc[missing_shop_category_rows, 'pred'] = 0\n",
    "\n",
    "# Create the submission file:\n",
    "\n",
    "submission = data_load['sample_submission'].copy()\n",
    "\n",
    "submission.loc[:, 'item_cnt_month'] = predictions.pred\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUBMISSION_FILE = \"../data/sales_sub_20180128g.csv\"\n",
    "\n",
    "submission.to_csv(SUBMISSION_FILE, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding (optional, only for DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding of shop_id\n",
    "# The test set has only a few shops, so we have to use scikitlearn onehotencoder\n",
    "\n",
    "cols = ['shop_id']\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# FIT\n",
    "enc.fit(Data['train'][cols])\n",
    "\n",
    "# Transform\n",
    "for set_name in ['train', 'test']:\n",
    "    vec_data= pd.DataFrame(enc.transform(Data[set_name][cols]).toarray())\n",
    "    vec_data.columns = [\"shop_id_\" + str(i) for i in range(enc.feature_indices_[1])]\n",
    "    vec_data.index = Data[set_name].index\n",
    "    Data[set_name] = Data[set_name].drop(cols, axis=1)\n",
    "    Data[set_name] = Data[set_name].join(vec_data)\n",
    "    \n",
    "# One hot encoding of item_category_id\n",
    "# The test set has only a few shops, so we have to use scikitlearn onehotencoder\n",
    "\n",
    "cols = ['item_category_id']\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# FIT\n",
    "enc.fit(Data['train'][cols])\n",
    "\n",
    "# Transform\n",
    "for set_name in ['train', 'test']:\n",
    "    vec_data= pd.DataFrame(enc.transform(Data[set_name][cols]).toarray())\n",
    "    vec_data.columns = [\"item_category_id_\" + str(i) for i in range(enc.feature_indices_[1])]\n",
    "    vec_data.index = Data[set_name].index\n",
    "    Data[set_name] = Data[set_name].drop(cols, axis=1)\n",
    "    Data[set_name] = Data[set_name].join(vec_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "sales_predictions = pd.DataFrame(Data['evaluation'].item_cnt_month.copy())\n",
    "sales_predictions.item_cnt_month = 0.28\n",
    "\n",
    "\n",
    "\n",
    "mse = mean_squared_error(Data['evaluation'].item_cnt_month, sales_predictions.item_cnt_month)\n",
    "rmse_test = np.sqrt(mse)\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data['train']['item_cnt_month_minus_12'] = 0\n",
    "Data['train']['item_cnt_month_minus_3'] = 0\n",
    "Data['train']['item_cnt_month_minus_1'] = 0 \n",
    "\n",
    "for month in range(12, 33,1):\n",
    "    condition = Data['train'].date_block_num == month\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_12'] = Data['train'][Data['train'].date_block_num == (month-12)]\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_3'] = Data['train'][Data['train'].date_block_num == (month-3)]\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_1'] = Data['train'][Data['train'].date_block_num == (month-1)]\n",
    "\n",
    "if training:\n",
    "    Data['evaluation']['item_cnt_month_minus_12'] = 0\n",
    "    Data['evaluation']['item_cnt_month_minus_3'] = 0\n",
    "    Data['evaluation']['item_cnt_month_minus_1'] = 0 \n",
    "\n",
    "    Data['evaluation'].loc[:, 'item_cnt_month_minus_12'] = Data['train'][Data['train'].date_block_num == (33-12)]\n",
    "    Data['evaluation'].loc[:, 'item_cnt_month_minus_3'] = Data['train'][Data['train'].date_block_num == (33-3)]\n",
    "    Data['evaluation'].loc[:, 'item_cnt_month_minus_1'] = Data['train'][Data['train'].date_block_num == (33-1)]    \n",
    "\n",
    "# Merge version for evaluation and test\n",
    "Data['test']['item_cnt_month_minus_12'] = 0\n",
    "Data['test']['item_cnt_month_minus_3'] = 0\n",
    "Data['test']['item_cnt_month_minus_1'] = 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
