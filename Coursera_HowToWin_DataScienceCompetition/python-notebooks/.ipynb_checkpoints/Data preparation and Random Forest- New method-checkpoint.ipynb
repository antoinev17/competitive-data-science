{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "* Step 1: prepare the training and evaluation data set\n",
    "* Step 2: training with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_extraction\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "# To use part or all the train set\n",
    "training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load all Files (hey must be in input directory in a brother directory of the notebook)\n",
    "data_load = {\n",
    "    'item_categories': pd.read_csv('../input/item_categories.csv'), \n",
    "    'items': pd.read_csv('../input/items.csv'), \n",
    "    'sales_train': pd.read_csv('../input/sales_train_v2.csv'),\n",
    "    'sample_submission': pd.read_csv('../input/sample_submission.csv'),\n",
    "    'shops': pd.read_csv('../input/shops.csv'),\n",
    "    'test': pd.read_csv('../input/test.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {}\n",
    "\n",
    "# Sales data \n",
    "data_load['sales_train']['date'] = pd.to_datetime(data_load['sales_train']['date'], format = \"%d.%m.%Y\")\n",
    "\n",
    "#Data['sales']['day'] = transactions['date'].dt.day\n",
    "data_load['sales_train']['month'] = data_load['sales_train']['date'].dt.month\n",
    "data_load['sales_train']['year'] = data_load['sales_train']['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_load['sales_train'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Data['train'][( Data['train'].shop_id == 5) & ( Data['train'].item_id == 485)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training/evaluation set, with similar pattern as the test set:\n",
    "* All shops in both sets\n",
    "* evaluation set on the last month\n",
    "* unknown items in the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the evaluation set\n",
      "sizes: (31531, 5) (1577593, 5)\n"
     ]
    }
   ],
   "source": [
    "Data['train'] = data_load['sales_train'].groupby(['date_block_num', 'shop_id', 'item_id'], as_index = False).agg({\n",
    "    'item_price': np.mean,\n",
    "    'item_cnt_day': np.sum\n",
    "}).rename(columns = {'item_cnt_day': 'item_shop_count',\n",
    "          'item_price': 'item_shop_price' })\n",
    "\n",
    "if training: \n",
    "    \n",
    "    # Split on date to create the evaluation set\n",
    "    \n",
    "    print(\"Preparing the evaluation set\")\n",
    "    \n",
    "    condition = Data['train']['date_block_num']==33\n",
    "    Data['evaluation'] = Data['train'][condition]\n",
    "    Data['train'] = Data['train'][~condition]\n",
    "    \n",
    "    print(\"sizes:\" ,Data['evaluation'].shape, Data['train'].shape)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Prepare the test set\n",
    "    \n",
    "    print(\"Preparing the test set\")\n",
    "\n",
    "    Data['test'] = data_load['test'].copy()\n",
    "    Data['test']['month'] = 11\n",
    "    Data['test']['year'] = 2015\n",
    "    Data['test']['date_block_num'] = 34\n",
    "\n",
    "    Data['test'] = Data['test'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Data['train'][( Data['train'].shop_id == 5) & ( Data['train'].item_id == 485)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winsorization - on training set only to evaluate the effect on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_limit = Data['train'].item_shop_price.quantile([0.0, 0.999])[0.999]\n",
    "prices = Data['train'].item_shop_price\n",
    "Data['train'].loc[(prices > price_limit), 'item_shop_price'] = price_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the missing rows for train set...\n",
      "Adding the missing rows for evaluation set...\n"
     ]
    }
   ],
   "source": [
    "if training:\n",
    "    set_list = ['train', 'evaluation']\n",
    "else:\n",
    "    set_list = ['train']\n",
    "    \n",
    "months = data_load['sales_train'].groupby(['month', 'year'], as_index = False).date_block_num.first()\n",
    "\n",
    "for set_name in set_list:\n",
    "\n",
    "    # Add the missing rows date_block_num,shop_id, item_id with item_cnt_month = 0\n",
    "    \n",
    "    print('Adding the missing rows for', set_name, 'set...')\n",
    "    \n",
    "    # From the assignment of week 3\n",
    "    # It differs from old method, because we do not consider the shops without sales or the items not sold during a month\n",
    "    # whereas before we were considering all combinations, and afterwards we were removing the couple shop, item\n",
    "    # with no sales on the full period: that means 10M rows vs 14M and a mean value of 0.3343 which is too high compare to \n",
    "    # the test set: 0.28\n",
    "    # However, it solves the issue of items not sold for months impacting the average\n",
    "    index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = [] \n",
    "    for block_num in Data[set_name]['date_block_num'].unique():\n",
    "        cur_shops = Data[set_name][Data[set_name]['date_block_num']==block_num]['shop_id'].unique()\n",
    "        cur_items = Data[set_name][Data[set_name]['date_block_num']==block_num]['item_id'].unique()\n",
    "        grid.append(np.array(list(itertools.product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "    #turn the grid into pandas dataframe\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "    #join aggregated data to the grid\n",
    "    Data[set_name] = pd.merge(grid,Data[set_name],how='left',on=index_cols)\n",
    "    #sort the data\n",
    "    Data[set_name].sort_values(['date_block_num','shop_id','item_id'],inplace=True)\n",
    "\n",
    "    # Add the month and year\n",
    "    \n",
    "    Data[set_name] = Data[set_name].merge(months,\n",
    "                   how = 'left',\n",
    "                   on = ['date_block_num'])\n",
    "    \n",
    "    # For new rows missing values:\n",
    "    \n",
    "    Data[set_name].item_shop_count = Data[set_name].item_shop_count.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding lag: 1\n",
      "Adding lag: 3\n",
      "Adding lag: 6\n",
      "Adding lag: 12\n"
     ]
    }
   ],
   "source": [
    "# From http://mlwhiz.com/blog/2017/12/26/How_to_win_a_data_science_competition/\n",
    "\n",
    "lag_variables  = ['item_shop_count', 'item_shop_price']\n",
    "lags = [1, 3 , 6 , 12]\n",
    "for lag in lags:\n",
    "    \n",
    "    print('Adding lag:', lag)\n",
    "    \n",
    "    train_df = Data['train'].copy()\n",
    "    train_df.date_block_num+=lag\n",
    "    train_df = train_df[['date_block_num','shop_id','item_id']+lag_variables]\n",
    "    train_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n",
    "    Data['train'] = pd.merge(Data['train'], train_df,on=['date_block_num','shop_id','item_id'] ,how='left')\n",
    "    \n",
    "    if training:\n",
    "        Data['evaluation'] = pd.merge(Data['evaluation'], train_df,on=['date_block_num','shop_id','item_id'] ,how='left')\n",
    "    else:\n",
    "        Data['test'] = pd.merge(Data['test'], train_df,on=['date_block_num','shop_id','item_id'] ,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['train'][-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Data['train'].copy()\n",
    "b = Data['evaluation'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data['train'] = a.copy()\n",
    "Data['evaluation'] = b.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mean_encoding(enc_cols, \n",
    "                      mean_encodings, \n",
    "                      label_suffix, \n",
    "                      price_rename, \n",
    "                      count_rename, \n",
    "                      train_set, \n",
    "                      test_set):\n",
    "             \n",
    "    # Create the encoding on the train set\n",
    "    tmp = Data[train_set].groupby(enc_cols, as_index = False).agg(mean_encodings)\\\n",
    "        .rename(columns = {'item_shop_price': price_rename,\n",
    "                            'item_shop_count': count_rename})\n",
    "    tmp.columns = [col[0] if col[-1]=='' else col[0] + '_' + col[-1] + label_suffix for col in tmp.columns.values]\n",
    "\n",
    "    # Add the encoding on each set\n",
    " \n",
    "    Data[train_set] = Data[train_set].merge(tmp,\n",
    "            how = 'left',\n",
    "            on = enc_cols\n",
    "            )\n",
    "    \n",
    "    Data[test_set] =  Data[test_set].merge(tmp,\n",
    "            how = 'left',\n",
    "            on = enc_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding item category stats\n",
      "Adding shop item stats\n",
      "Adding overall item stats\n",
      "Adding overall shop stats\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Measures to use for the mean encoding\n",
    "mean_encodings = {'item_shop_price':['max','mean'], 'item_shop_count':['max', 'mean']}\n",
    "\n",
    "if training:\n",
    "    test_set = 'evaluation'\n",
    "else:\n",
    "    test_set = 'test'\n",
    "    \n",
    "for set_name in ['train', test_set]:\n",
    "    Data[set_name] = Data[set_name].merge(data_load['items'],\n",
    "            how = 'left',\n",
    "            on = 'item_id',\n",
    "            ).drop(['item_name'], axis = 1)\n",
    "\n",
    "### Add the mean encoding per item_category\n",
    "\n",
    "print(\"Adding shop category stats\")\n",
    "\n",
    "add_mean_encoding(enc_cols=['shop_id', 'item_category_id'],\n",
    "                  mean_encodings = mean_encodings,\n",
    "                  label_suffix='_over_months',\n",
    "                  price_rename='category_shop_price',\n",
    "                  count_rename='category_shop_count',\n",
    "                  train_set = 'train',\n",
    "                  test_set = test_set)\n",
    "    \n",
    "# Add the average price per item and shop, over the train data set (average price will be missing for some items)\n",
    "\n",
    "print(\"Adding shop item stats\")\n",
    "\n",
    "add_mean_encoding(enc_cols=['shop_id', 'item_id'],\n",
    "                  mean_encodings = mean_encodings,\n",
    "                  label_suffix='_over_months',\n",
    "                  price_rename='item_shop_price',\n",
    "                  count_rename='item_shop_count',\n",
    "                  train_set = 'train',\n",
    "                  test_set = test_set)\n",
    "\n",
    "# Add the average price per item over the train data set (average price will be missing for some items)\n",
    "\n",
    "print(\"Adding overall item stats\")  \n",
    "\n",
    "add_mean_encoding(enc_cols=['shop_id', 'item_id'],\n",
    "                  mean_encodings = mean_encodings,\n",
    "                  label_suffix='_over_all',\n",
    "                  price_rename='item_price',\n",
    "                  count_rename='item_count',\n",
    "                  train_set = 'train',\n",
    "                  test_set = test_set)\n",
    "\n",
    "# Add the mean encodings per shop over all\n",
    "\n",
    "print(\"Adding overall shop stats\")   \n",
    "\n",
    "add_mean_encoding(enc_cols=['shop_id', 'item_id'],\n",
    "                  mean_encodings = mean_encodings,\n",
    "                  label_suffix='_over_all',\n",
    "                  price_rename='shop_price',\n",
    "                  count_rename='shop_count',\n",
    "                  train_set = 'train',\n",
    "                  test_set = test_set)\n",
    "\n",
    "# Remove the item_id\n",
    "\n",
    "for set_name in set_list:\n",
    "    Data[set_name] = Data[set_name].drop(['item_id', 'item_category_id', 'shop_id'], axis = 1)\n",
    "    \n",
    "Data['train'] = Data['train'].drop(['item_shop_price'], axis = 1)\n",
    "if training:\n",
    "    Data['evaluation'] = Data['evaluation'].drop(['item_shop_price'], axis = 1)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder the columns alphabetically to avoid issues with columns position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    set_list = ['train','evaluation']\n",
    "else:\n",
    "    set_list = ['train','test']\n",
    "\n",
    "for set_name in set_list:\n",
    "    Data[set_name].sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "case = '20180201a'\n",
    "\n",
    "DATA_LEARNING_FILE = \"../data/sales-\" + case\n",
    "DATA_EVALUATION_FILE = \"../data/evaluation-\" + case\n",
    "DATA_TEST_FILE = \"../data/test-\" + case\n",
    "\n",
    "Data['train'].to_pickle(DATA_LEARNING_FILE)\n",
    "if training:\n",
    "    Data['evaluation'].to_pickle(DATA_EVALUATION_FILE)\n",
    "else:\n",
    "    Data['test'].to_pickle(DATA_TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 20180127c : with windsorization 0.999 , removing rows for store, item without any sale\n",
    "# 20180127d : idem without windsorization \n",
    "# 20180127e : idem with windsorization 0.9999\n",
    "# 20180127f : idem with windsorization 0.99\n",
    "# 20180127g : idem with windsorization 0.99, with full set\n",
    "# 20180127h : idem with windsorization 0.999, with full set\n",
    "# 20180128c : windsorization on train only 0.999, with train/eval split\n",
    "# 20180128d : windsorization on train only 0.99, with train/eval split\n",
    "# 20180128e : no windsorization, with train/eval split\n",
    "# 20180128f : with windsor 0.999 on price, with train/eval split\n",
    "# 20180128g : with windsor 0.999 on price, without eval\n",
    "# 201801230a : with windsor 0.999 on price, with eval, with lagged item_count, shop_id, item_category_id\n",
    "# 201801230b : with windsor 0.999 on price, without eval, with lagged item_count, shop_id, item_category_id\n",
    "# 20180201a : with new mean encoding, lagged features, no shop,item, category ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart it to retrieve data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "training = True\n",
    "\n",
    "case = '20180201a'\n",
    "\n",
    "DATA_LEARNING_FILE = \"../data/sales-\" + case\n",
    "DATA_EVALUATION_FILE = \"../data/evaluation-\" + case\n",
    "\n",
    "Data = {}\n",
    "\n",
    "Data['train'] = pd.read_pickle(DATA_LEARNING_FILE)\n",
    "if training:\n",
    "    Data['evaluation'] = pd.read_pickle(DATA_EVALUATION_FILE)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data['train'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipping  = True\n",
    "\n",
    "# Random split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#train_set, test_set = train_test_split(Data['train'], test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Remove the first year to decrease training time (and after we will add delayed values)\n",
    "Data['train'] = Data['train'][Data['train'].date_block_num >11].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x_train = Data['train'].drop(['item_shop_count'], axis = 1)\n",
    "\n",
    "if clipping:\n",
    "    y_train = Data['train'].item_shop_count.clip(0,20)\n",
    "else:\n",
    "    y_train = Data['train'].item_shop_count\n",
    "\n",
    "# I should remove the evaluation prediction rows with missings category\n",
    "\n",
    "if training:\n",
    "    x_eval = Data['evaluation'].drop(['item_shop_count'], axis = 1)\n",
    "    y_eval = Data['evaluation'].item_shop_count\n",
    "\n",
    "del(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_shop_count_max_over_months</th>\n",
       "      <th>category_shop_count_mean_over_months</th>\n",
       "      <th>category_shop_price_max_over_months</th>\n",
       "      <th>category_shop_price_mean_over_months</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_count_max_over_all</th>\n",
       "      <th>item_count_mean_over_all</th>\n",
       "      <th>item_price_max_over_all</th>\n",
       "      <th>item_price_mean_over_all</th>\n",
       "      <th>item_shop_count_lag_1</th>\n",
       "      <th>...</th>\n",
       "      <th>item_shop_price_lag_3</th>\n",
       "      <th>item_shop_price_lag_6</th>\n",
       "      <th>item_shop_price_max_over_months</th>\n",
       "      <th>item_shop_price_mean_over_months</th>\n",
       "      <th>month</th>\n",
       "      <th>shop_count_max_over_all</th>\n",
       "      <th>shop_count_mean_over_all</th>\n",
       "      <th>shop_price_max_over_all</th>\n",
       "      <th>shop_price_mean_over_all</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>5.554142e+06</td>\n",
       "      <td>5.554142e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>3.750817e+06</td>\n",
       "      <td>3.750817e+06</td>\n",
       "      <td>4.898597e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>855295.000000</td>\n",
       "      <td>818884.000000</td>\n",
       "      <td>3.750817e+06</td>\n",
       "      <td>3.750817e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "      <td>3.750817e+06</td>\n",
       "      <td>3.750817e+06</td>\n",
       "      <td>6.186922e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.127808e+01</td>\n",
       "      <td>3.387926e-01</td>\n",
       "      <td>3.239089e+03</td>\n",
       "      <td>7.548547e+02</td>\n",
       "      <td>2.115780e+01</td>\n",
       "      <td>2.126528e+00</td>\n",
       "      <td>4.021299e-01</td>\n",
       "      <td>8.536100e+02</td>\n",
       "      <td>7.665754e+02</td>\n",
       "      <td>4.190263e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>847.795518</td>\n",
       "      <td>798.105608</td>\n",
       "      <td>8.536100e+02</td>\n",
       "      <td>7.665754e+02</td>\n",
       "      <td>5.798793e+00</td>\n",
       "      <td>2.126528e+00</td>\n",
       "      <td>4.021299e-01</td>\n",
       "      <td>8.536100e+02</td>\n",
       "      <td>7.665754e+02</td>\n",
       "      <td>2.014363e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.858849e+01</td>\n",
       "      <td>1.969598e+00</td>\n",
       "      <td>3.178090e+03</td>\n",
       "      <td>1.178192e+03</td>\n",
       "      <td>5.869434e+00</td>\n",
       "      <td>9.731047e+00</td>\n",
       "      <td>2.795261e+00</td>\n",
       "      <td>1.548723e+03</td>\n",
       "      <td>1.457555e+03</td>\n",
       "      <td>3.892317e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1562.323323</td>\n",
       "      <td>1448.916012</td>\n",
       "      <td>1.548723e+03</td>\n",
       "      <td>1.457555e+03</td>\n",
       "      <td>3.288499e+00</td>\n",
       "      <td>9.731047e+00</td>\n",
       "      <td>2.795261e+00</td>\n",
       "      <td>1.548723e+03</td>\n",
       "      <td>1.457555e+03</td>\n",
       "      <td>4.809363e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.000000e-01</td>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.000000e-01</td>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>2.014000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>6.407323e-02</td>\n",
       "      <td>9.990000e+02</td>\n",
       "      <td>2.762345e+02</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.290000e+02</td>\n",
       "      <td>2.017273e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>199.330000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>2.290000e+02</td>\n",
       "      <td>2.017273e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.290000e+02</td>\n",
       "      <td>2.017273e+02</td>\n",
       "      <td>2.014000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>1.534579e-01</td>\n",
       "      <td>2.299000e+03</td>\n",
       "      <td>3.977126e+02</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.695652e-02</td>\n",
       "      <td>3.990000e+02</td>\n",
       "      <td>3.490000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>3.990000e+02</td>\n",
       "      <td>3.490000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.695652e-02</td>\n",
       "      <td>3.990000e+02</td>\n",
       "      <td>3.490000e+02</td>\n",
       "      <td>2.014000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>3.439967e-01</td>\n",
       "      <td>3.799000e+03</td>\n",
       "      <td>9.857859e+02</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.333333e-01</td>\n",
       "      <td>9.900000e+02</td>\n",
       "      <td>8.266477e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>899.000000</td>\n",
       "      <td>9.900000e+02</td>\n",
       "      <td>8.266477e+02</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.333333e-01</td>\n",
       "      <td>9.900000e+02</td>\n",
       "      <td>8.266477e+02</td>\n",
       "      <td>2.015000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.644000e+03</td>\n",
       "      <td>4.702619e+02</td>\n",
       "      <td>2.249000e+04</td>\n",
       "      <td>2.231750e+04</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>1.644000e+03</td>\n",
       "      <td>6.501000e+02</td>\n",
       "      <td>2.249000e+04</td>\n",
       "      <td>2.249000e+04</td>\n",
       "      <td>1.305000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>22490.000000</td>\n",
       "      <td>22490.000000</td>\n",
       "      <td>2.249000e+04</td>\n",
       "      <td>2.249000e+04</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>1.644000e+03</td>\n",
       "      <td>6.501000e+02</td>\n",
       "      <td>2.249000e+04</td>\n",
       "      <td>2.249000e+04</td>\n",
       "      <td>2.015000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_shop_count_max_over_months  \\\n",
       "count                         6.186922e+06   \n",
       "mean                          3.127808e+01   \n",
       "std                           6.858849e+01   \n",
       "min                           0.000000e+00   \n",
       "25%                           5.000000e+00   \n",
       "50%                           1.300000e+01   \n",
       "75%                           2.900000e+01   \n",
       "max                           1.644000e+03   \n",
       "\n",
       "       category_shop_count_mean_over_months  \\\n",
       "count                          6.186922e+06   \n",
       "mean                           3.387926e-01   \n",
       "std                            1.969598e+00   \n",
       "min                            0.000000e+00   \n",
       "25%                            6.407323e-02   \n",
       "50%                            1.534579e-01   \n",
       "75%                            3.439967e-01   \n",
       "max                            4.702619e+02   \n",
       "\n",
       "       category_shop_price_max_over_months  \\\n",
       "count                         5.554142e+06   \n",
       "mean                          3.239089e+03   \n",
       "std                           3.178090e+03   \n",
       "min                           3.000000e+00   \n",
       "25%                           9.990000e+02   \n",
       "50%                           2.299000e+03   \n",
       "75%                           3.799000e+03   \n",
       "max                           2.249000e+04   \n",
       "\n",
       "       category_shop_price_mean_over_months  date_block_num  \\\n",
       "count                          5.554142e+06    6.186922e+06   \n",
       "mean                           7.548547e+02    2.115780e+01   \n",
       "std                            1.178192e+03    5.869434e+00   \n",
       "min                            3.000000e+00    1.200000e+01   \n",
       "25%                            2.762345e+02    1.600000e+01   \n",
       "50%                            3.977126e+02    2.100000e+01   \n",
       "75%                            9.857859e+02    2.600000e+01   \n",
       "max                            2.231750e+04    3.200000e+01   \n",
       "\n",
       "       item_count_max_over_all  item_count_mean_over_all  \\\n",
       "count             6.186922e+06              6.186922e+06   \n",
       "mean              2.126528e+00              4.021299e-01   \n",
       "std               9.731047e+00              2.795261e+00   \n",
       "min               0.000000e+00             -2.000000e-01   \n",
       "25%               0.000000e+00              0.000000e+00   \n",
       "50%               1.000000e+00              8.695652e-02   \n",
       "75%               2.000000e+00              3.333333e-01   \n",
       "max               1.644000e+03              6.501000e+02   \n",
       "\n",
       "       item_price_max_over_all  item_price_mean_over_all  \\\n",
       "count             3.750817e+06              3.750817e+06   \n",
       "mean              8.536100e+02              7.665754e+02   \n",
       "std               1.548723e+03              1.457555e+03   \n",
       "min               9.000000e-02              9.000000e-02   \n",
       "25%               2.290000e+02              2.017273e+02   \n",
       "50%               3.990000e+02              3.490000e+02   \n",
       "75%               9.900000e+02              8.266477e+02   \n",
       "max               2.249000e+04              2.249000e+04   \n",
       "\n",
       "       item_shop_count_lag_1      ...       item_shop_price_lag_3  \\\n",
       "count           4.898597e+06      ...               855295.000000   \n",
       "mean            4.190263e-01      ...                  847.795518   \n",
       "std             3.892317e+00      ...                 1562.323323   \n",
       "min            -2.000000e+00      ...                    0.500000   \n",
       "25%             0.000000e+00      ...                  199.330000   \n",
       "50%             0.000000e+00      ...                  399.000000   \n",
       "75%             0.000000e+00      ...                  959.000000   \n",
       "max             1.305000e+03      ...                22490.000000   \n",
       "\n",
       "       item_shop_price_lag_6  item_shop_price_max_over_months  \\\n",
       "count          818884.000000                     3.750817e+06   \n",
       "mean              798.105608                     8.536100e+02   \n",
       "std              1448.916012                     1.548723e+03   \n",
       "min                 0.100000                     9.000000e-02   \n",
       "25%               199.000000                     2.290000e+02   \n",
       "50%               399.000000                     3.990000e+02   \n",
       "75%               899.000000                     9.900000e+02   \n",
       "max             22490.000000                     2.249000e+04   \n",
       "\n",
       "       item_shop_price_mean_over_months         month  \\\n",
       "count                      3.750817e+06  6.186922e+06   \n",
       "mean                       7.665754e+02  5.798793e+00   \n",
       "std                        1.457555e+03  3.288499e+00   \n",
       "min                        9.000000e-02  1.000000e+00   \n",
       "25%                        2.017273e+02  3.000000e+00   \n",
       "50%                        3.490000e+02  6.000000e+00   \n",
       "75%                        8.266477e+02  8.000000e+00   \n",
       "max                        2.249000e+04  1.200000e+01   \n",
       "\n",
       "       shop_count_max_over_all  shop_count_mean_over_all  \\\n",
       "count             6.186922e+06              6.186922e+06   \n",
       "mean              2.126528e+00              4.021299e-01   \n",
       "std               9.731047e+00              2.795261e+00   \n",
       "min               0.000000e+00             -2.000000e-01   \n",
       "25%               0.000000e+00              0.000000e+00   \n",
       "50%               1.000000e+00              8.695652e-02   \n",
       "75%               2.000000e+00              3.333333e-01   \n",
       "max               1.644000e+03              6.501000e+02   \n",
       "\n",
       "       shop_price_max_over_all  shop_price_mean_over_all          year  \n",
       "count             3.750817e+06              3.750817e+06  6.186922e+06  \n",
       "mean              8.536100e+02              7.665754e+02  2.014363e+03  \n",
       "std               1.548723e+03              1.457555e+03  4.809363e-01  \n",
       "min               9.000000e-02              9.000000e-02  2.014000e+03  \n",
       "25%               2.290000e+02              2.017273e+02  2.014000e+03  \n",
       "50%               3.990000e+02              3.490000e+02  2.014000e+03  \n",
       "75%               9.900000e+02              8.266477e+02  2.015000e+03  \n",
       "max               2.249000e+04              2.249000e+04  2.015000e+03  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def model_evaluation(model_reg, x_train, y_train, x_test, y_test, use_average = True): \n",
    "    sales_predictions = model_reg.predict(x_train.fillna(-999))\n",
    "    mse = mean_squared_error(y_train, sales_predictions)\n",
    "    rmse_train = np.sqrt(mse)\n",
    "\n",
    "    sales_predictions = pd.DataFrame({'pred': model_reg.predict(x_test.fillna(-999))})\n",
    "    \n",
    "    if use_average:\n",
    "        # replace the shop, item rows with no values, with the average on the category \n",
    "        missing_shop_item_rows = pd.isnull(x_test.item_shop_price_mean_over_months)\n",
    "        print('Missing lines for shop,items: ', len(x_test[missing_shop_item_rows]))\n",
    "        sales_predictions.loc[missing_shop_item_rows, 'pred'] = x_test[missing_shop_item_rows].category_shop_count_mean_over_months\n",
    "    \n",
    "        # replace the shop, category with no values, with 0 (the shop is not selling this category)\n",
    "        missing_shop_category_rows = pd.isnull(sales_predictions.pred)  \n",
    "        print('Missing lines for shop,category: ', len(x_test[missing_shop_category_rows]))\n",
    "        sales_predictions.loc[missing_shop_category_rows, 'pred'] = 0\n",
    "    \n",
    "    mse = mean_squared_error(y_test.clip(0,20), sales_predictions.pred.clip(0,20))\n",
    "    rmse_test = np.sqrt(mse)\n",
    "\n",
    "    print(\"train error: \", '{0:.3f}'.format(rmse_train), \"evaluation error: \", '{0:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "           max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=25, n_jobs=5, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df_reg = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
    "           max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
    "           n_estimators=25, n_jobs=5, oob_score=False, random_state=None,\n",
    "           verbose=0, warm_start=False)\n",
    "df_reg.fit(x_train.fillna(-999), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing lines for shop,items:  121115\n",
      "Missing lines for shop,category:  5559\n",
      "train error:  0.537 evaluation error:  1.011\n"
     ]
    }
   ],
   "source": [
    "model_evaluation(df_reg, x_train, y_train, x_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category_shop_count_max_over_months',\n",
       "       'category_shop_count_mean_over_months',\n",
       "       'category_shop_price_max_over_months',\n",
       "       'category_shop_price_mean_over_months', 'date_block_num',\n",
       "       'item_count_max_over_all', 'item_count_mean_over_all',\n",
       "       'item_price_max_over_all', 'item_price_mean_over_all',\n",
       "       'item_shop_count_lag_1', 'item_shop_count_lag_12',\n",
       "       'item_shop_count_lag_3', 'item_shop_count_lag_6',\n",
       "       'item_shop_count_max_over_months', 'item_shop_count_mean_over_months',\n",
       "       'item_shop_price_lag_1', 'item_shop_price_lag_12',\n",
       "       'item_shop_price_lag_3', 'item_shop_price_lag_6',\n",
       "       'item_shop_price_max_over_months', 'item_shop_price_mean_over_months',\n",
       "       'month', 'shop_count_max_over_all', 'shop_count_mean_over_all',\n",
       "       'shop_price_max_over_all', 'shop_price_mean_over_all', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_eval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with 20180127c\n",
    "# max_depth=15, n_estimators=20, max_features: 3 : 1.004 / 1.403\n",
    "# max_depth=20, n_estimators=20, max_features: 2 : 0.839 / 1.399\n",
    "# max_depth=20, n_estimators=20, max_features: 3 : 0.818 / 1.394\n",
    "# max_depth=20, n_estimators=50, max_features: 3 : 0.834 / 1.391 > best\n",
    "# max_depth=25, n_estimators=20, max_features: 3 : 0.657 / 1.404\n",
    "# max_depth=20, n_estimators=20, max_features: 4 : 0.795 / 1.410\n",
    "\n",
    "# with 20180127d, max_depth=20, n_estimators=50, max_features: 3\n",
    "# train error: 1.104 evaluation error:  4.596 > windsorization is essential!!\n",
    "\n",
    "# with 20180127e, max_depth=20, n_estimators=25, max_features: 3\n",
    "# train error:  0.949 evaluation error:  1.612 still not as good\n",
    "\n",
    "# with 20180127f, max_depth=20, n_estimators=25, max_features: 3\n",
    "# train error:  0.696 evaluation error:  1.027 > best but beware, \n",
    "#I may just be reducing the variance of the evaluation set :)  \n",
    "# let check with the Kaggle :) \n",
    "# with clip 0-20 train error:  0.692 evaluation error:  0.924 (meaningless as it was windsored)\n",
    "\n",
    "# with 20180127h, max_depth=20, n_estimators=50, max_features: 3, windsor:0.999, full set\n",
    "# kaggle: 1.41\n",
    "\n",
    "# with 20180127g, max_depth=20, n_estimators=50, max_features: 3, windsor:0.99, full set\n",
    "# kaggle: 1.40 > best\n",
    "\n",
    "# Conclusion: we have brought the results much closer 0.7 / 1.0 / 1.4\n",
    "# but we are still higher than before ??? \n",
    "# does the category do help or harm?\n",
    "\n",
    "# with 20180127f, removing category stat, max_depth=20, n_estimators=25, max_features: 3, windsor:0.99, full set\n",
    "# 0.677 evaluation error:  1.043 : slightly worse\n",
    "\n",
    "# with 20180128d, max_depth=20, n_estimators=25, max_features: 3, windsor on train 0.99\n",
    "# train error:  0.601 evaluation error:  5.237\n",
    "# with clipping 0-20: train error:  0.607 evaluation error:  0.979\n",
    "\n",
    "# with 20180128c, max_depth=20, n_estimators=25, max_features: 3, windsor on train 0.999\n",
    "# train error:  0.743 evaluation error:  5.039\n",
    "# with clipping 0-20: train error:  0.743 evaluation error:  0.996\n",
    "\n",
    "# with 20180128e, max_depth=20, n_estimators=25, max_features: 3, no windsor on train\n",
    "# train error:  1.095 evaluation error:  4.702\n",
    "# with clipping 0-20 at evaluation (y_eval and pred): train error:  1.094 evaluation error:  1.002\n",
    "# with clipping 0-20 of y_train train error:  0.616 evaluation error:  0.975 > best\n",
    "\n",
    "# with 20180128f, max_depth=20, n_estimators=25, max_features: 3,  windsor 0.999 on price only, clipping 0-20 on train\n",
    "# train error:  0.616 evaluation error:  0.973 > best\n",
    "\n",
    "# with 20180128g, max_depth=20, n_estimators=100, max_features: 3,  windsor 0.999 on price only, clipping 0-20 on train\n",
    "# no eval\n",
    "# kaggle: 1.02119\n",
    "\n",
    "# with 20180130a, max_depth=20, n_estimators=25, max_features: 3,  with lagged features, item_id, category_item_id\n",
    "# 0.567 evaluation error:  0.962 > overfitted...\n",
    "\n",
    "# with 20180130b, max_depth=20, n_estimators=25, max_features: 3,  with lagged features, without item_id, category_item_id\n",
    "# 0.579 evaluation error:  0.968 > actually slightly worse\n",
    "# Kaggle:\n",
    "\n",
    "# with 20180201a, train error:  0.537 evaluation error:  1.011 > overfit\n",
    "# idem Xgboost eta = 0.3,max_depth=4, n_estimators=300, learning_rate=0.05:  0.876 evaluation error:  0.977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(df_reg, '../models/randomforest_20180128g.pkl')\n",
    "#df_reg = joblib.load('../models/randomforest_20180127g.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = df_reg.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# You can experiment with many other options here, using the same .fit() and .predict()\n",
    "# methods; see http://scikit-learn.org\n",
    "# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\n",
    "\n",
    "eval_set = [(x_eval, y_eval)]\n",
    "\n",
    "gbm = xgb.XGBRegressor(eta = 0.3,\n",
    "                       max_depth=5, \n",
    "                       n_estimators=300, \n",
    "                       learning_rate=0.05,\n",
    "                       n_jobs = 5).fit(x_train, y_train, eval_metric=\"rmse\", eval_set=eval_set, verbose=True)\n",
    "\n",
    "if training:\n",
    "    model_evaluation(gbm, x_train, y_train, x_eval, y_eval, use_average = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [10,50, 100], 'max_features': [2,4,6], 'max_depth': [5, 10, 20]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, n_jobs = 4,\n",
    "                          scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_evaluation(grid_search.best_estimator_, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt_reg = GradientBoostingRegressor(learning_rate = 0.1)\n",
    "gbrt_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "x_test =  test_set.drop(['item_cnt_month'], axis = 1)\n",
    "y_test = test_set.item_cnt_month\n",
    "\n",
    "model_evaluation(gbrt_reg, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 20180120-01 with default randomForestRegressor, only 4 features, eval: 4.90 (not better through grid search)\n",
    "* idem, but adding the item_category, eval: 5.4 vs 2.42 (train set)\n",
    "* with GBRT, eval = 7.1 vs 7.36(train set) > not working\n",
    "* with Random Forest, adding the min, max, mean of sales per store,item eval 4.86 vs 2.04\n",
    "* idem, removing the item_id: train error:  2.16 test_error:  4.64\n",
    "* idem, selecting the best estimator: train error:  1.93 test_error:  4.56\n",
    "* adding the average price per item, per shop and overall, max_features 3, n_estimators=50 train error:  train error:  1.98 test_error:  4.45 (best)\n",
    "* with one hot encoding of the shop_id: train error:  1.97 test_error:  4.90\n",
    "* with one hot encoding of the item_category_id: train error:  2.07 test_error:  4.79\n",
    "* with windsorization of the count_item_day, and the price the results improve a lot: train error:  1.08 test_error:  2.91\n",
    "* adding the 0 values: train error:  0.33 test_error:  0.94 > kaggle 1.27 without putting missing items to 0 this does not change a thing actually)\n",
    "* item category is useless > split and provide some price per category\n",
    "* with new train/test set, not removing the new item prediction: train error:  0.34 test_error:  1.94 / Kaggle: 2.61 (I did not remove the 0 prediction there, nor retrain on the full set)\n",
    "* with category stats randomforest_20180126b: tain_error:  0.35 test_error:  1.75 Kaggle: 2.4 \n",
    "* idem, removing missing items: Kaggle: 2.42 it is worse, removing missing category is better!\n",
    "* with full training: Kaggle: 2.58!!! maybe I am just tragically overfitting\n",
    "* trying to reduce overfitting by putting max_features = 2/ min_samples_leaf=2: train error:  0.657 test_error:  1.805\n",
    "* removing the mean, removing the artificial item deletion from validation set: train error:  0.339 evaluation error:  1.768 Kaggle: \n",
    "\n",
    "* with removal of missing shop, item rows, and replacement by avrage and 0, 100 estimators: train error:  0.400 evaluation error:  1.417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Submission preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "DATA_TEST_FILE = \"../data/test-201801230b\"\n",
    "\n",
    "Data = {}\n",
    "\n",
    "Data['test'] = pd.read_pickle(DATA_TEST_FILE)\n",
    "\n",
    "Data['test'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_average = True\n",
    "\n",
    "X_test = Data['test'].drop(['ID'], axis = 1)\n",
    "\n",
    "# Option 1\n",
    "# It trust the model will learn from the category count for the missing item - \n",
    "# for the missing category, I set the prediction to 0\n",
    "# missing_shop_item_indices = pd.isnull(Data['test']['item_cnt_month_mean'])\n",
    "# This is not improving the score!\n",
    "\n",
    "predictions = pd.DataFrame({'pred': df_reg.predict(X_test.fillna(0)).clip(0,20)})\n",
    "\n",
    "# replace the shop, item rows with no values, with the average on the category\n",
    "\n",
    "if use_average:\n",
    "        # replace the shop, item rows with no values, with the average on the category \n",
    "        missing_shop_item_rows = pd.isnull(X_test.item_shop_mean_price)\n",
    "        print('Missing lines for shop,items: ', len(X_test[missing_shop_item_rows]))\n",
    "        predictions.loc[missing_shop_item_rows, 'pred'] = X_test[missing_shop_item_rows].category_cnt_month_mean\n",
    "\n",
    "# for the shop with not category, replace with \n",
    "\n",
    "missing_shop_category_rows = pd.isnull(predictions.pred)\n",
    "print('Missing lines for shop,category: ', len(X_test[missing_shop_category_rows]))\n",
    "predictions.loc[missing_shop_category_rows, 'pred'] = 0\n",
    "\n",
    "# Create the submission file:\n",
    "\n",
    "submission = data_load['sample_submission'].copy()\n",
    "\n",
    "submission.loc[:, 'item_cnt_month'] = predictions.pred\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUBMISSION_FILE = \"../data/sales_sub_20180130b.csv\"\n",
    "\n",
    "submission.to_csv(SUBMISSION_FILE, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding (optional, only for DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding of shop_id\n",
    "# The test set has only a few shops, so we have to use scikitlearn onehotencoder\n",
    "\n",
    "cols = ['shop_id']\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# FIT\n",
    "enc.fit(Data['train'][cols])\n",
    "\n",
    "# Transform\n",
    "for set_name in ['train', 'test']:\n",
    "    vec_data= pd.DataFrame(enc.transform(Data[set_name][cols]).toarray())\n",
    "    vec_data.columns = [\"shop_id_\" + str(i) for i in range(enc.feature_indices_[1])]\n",
    "    vec_data.index = Data[set_name].index\n",
    "    Data[set_name] = Data[set_name].drop(cols, axis=1)\n",
    "    Data[set_name] = Data[set_name].join(vec_data)\n",
    "    \n",
    "# One hot encoding of item_category_id\n",
    "# The test set has only a few shops, so we have to use scikitlearn onehotencoder\n",
    "\n",
    "cols = ['item_category_id']\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# FIT\n",
    "enc.fit(Data['train'][cols])\n",
    "\n",
    "# Transform\n",
    "for set_name in ['train', 'test']:\n",
    "    vec_data= pd.DataFrame(enc.transform(Data[set_name][cols]).toarray())\n",
    "    vec_data.columns = [\"item_category_id_\" + str(i) for i in range(enc.feature_indices_[1])]\n",
    "    vec_data.index = Data[set_name].index\n",
    "    Data[set_name] = Data[set_name].drop(cols, axis=1)\n",
    "    Data[set_name] = Data[set_name].join(vec_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "sales_predictions = pd.DataFrame(Data['evaluation'].item_cnt_month.copy())\n",
    "sales_predictions.item_cnt_month = 0.28\n",
    "\n",
    "\n",
    "\n",
    "mse = mean_squared_error(Data['evaluation'].item_cnt_month, sales_predictions.item_cnt_month)\n",
    "rmse_test = np.sqrt(mse)\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data['train']['item_cnt_month_minus_12'] = 0\n",
    "Data['train']['item_cnt_month_minus_3'] = 0\n",
    "Data['train']['item_cnt_month_minus_1'] = 0 \n",
    "\n",
    "for month in range(12, 33,1):\n",
    "    condition = Data['train'].date_block_num == month\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_12'] = Data['train'][Data['train'].date_block_num == (month-12)]\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_3'] = Data['train'][Data['train'].date_block_num == (month-3)]\n",
    "    Data['train'].loc[condition, 'item_cnt_month_minus_1'] = Data['train'][Data['train'].date_block_num == (month-1)]\n",
    "\n",
    "if training:\n",
    "    Data['evaluation']['item_cnt_month_minus_12'] = 0\n",
    "    Data['evaluation']['item_cnt_month_minus_3'] = 0\n",
    "    Data['evaluation']['item_cnt_month_minus_1'] = 0 \n",
    "\n",
    "    Data['evaluation'].loc[:, 'item_cnt_month_minus_12'] = Data['train'][Data['train'].date_block_num == (33-12)]\n",
    "    Data['evaluation'].loc[:, 'item_cnt_month_minus_3'] = Data['train'][Data['train'].date_block_num == (33-3)]\n",
    "    Data['evaluation'].loc[:, 'item_cnt_month_minus_1'] = Data['train'][Data['train'].date_block_num == (33-1)]    \n",
    "\n",
    "# Merge version for evaluation and test\n",
    "Data['test']['item_cnt_month_minus_12'] = 0\n",
    "Data['test']['item_cnt_month_minus_3'] = 0\n",
    "Data['test']['item_cnt_month_minus_1'] = 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
