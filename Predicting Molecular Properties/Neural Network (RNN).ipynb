{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "CREATE_SET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all Files (they must be in data directory in a brother directory of the notebook)\n",
    "data_load = {\n",
    "    'dipole_moments': pd.read_csv('./data/dipole_moments.csv'),\n",
    "    'magnetic_shielding_tensors': pd.read_csv('./data/magnetic_shielding_tensors.csv'),\n",
    "    'mulliken_charges': pd.read_csv('./data/mulliken_charges.csv'),\n",
    "    'potential_energy': pd.read_csv('./data/potential_energy.csv'),\n",
    "    'sample_submission': pd.read_csv('./data/sample_submission.csv'),\n",
    "    'scalar_coupling_contributions': pd.read_csv('./data/scalar_coupling_contributions.csv'),\n",
    "    'structures': pd.read_csv('./data/structures.csv'),\n",
    "    'train': pd.read_csv('./data/train.csv'), \n",
    "    'test': pd.read_csv('./data/test.csv')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = data_load['train']\n",
    "MAX_MOL_ATOMS_NB = max(train_all.atom_index_0.max(),train_all.atom_index_1.max()) + 1\n",
    "COUPLING_TYPE_NB = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>atom_C</th>\n",
       "      <th>atom_F</th>\n",
       "      <th>atom_H</th>\n",
       "      <th>atom_N</th>\n",
       "      <th>atom_O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index         x         y         z  atom_C  atom_F  \\\n",
       "0  dsgdb9nsd_000001           0 -0.012698  1.085804  0.008001       1       0   \n",
       "1  dsgdb9nsd_000001           1  0.002150 -0.006031  0.001976       0       0   \n",
       "2  dsgdb9nsd_000001           2  1.011731  1.463751  0.000277       0       0   \n",
       "3  dsgdb9nsd_000001           3 -0.540815  1.447527 -0.876644       0       0   \n",
       "4  dsgdb9nsd_000001           4 -0.523814  1.437933  0.906397       0       0   \n",
       "\n",
       "   atom_H  atom_N  atom_O  \n",
       "0       0       0       0  \n",
       "1       1       0       0  \n",
       "2       1       0       0  \n",
       "3       1       0       0  \n",
       "4       1       0       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirmation with the molecule structure\n",
    "structures = data_load['structures']\n",
    "\n",
    "structures_col = structures.columns\n",
    "\n",
    "\n",
    "# Create one_hot_encoding for the atom type\n",
    "structures = pd.get_dummies(structures, prefix = 'atom', columns = ['atom'])\n",
    "structures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>coupling_type_1JHC</th>\n",
       "      <th>coupling_type_1JHN</th>\n",
       "      <th>coupling_type_2JHC</th>\n",
       "      <th>coupling_type_2JHH</th>\n",
       "      <th>coupling_type_2JHN</th>\n",
       "      <th>coupling_type_3JHC</th>\n",
       "      <th>coupling_type_3JHH</th>\n",
       "      <th>coupling_type_3JHN</th>\n",
       "      <th>DM_X</th>\n",
       "      <th>DM_Y</th>\n",
       "      <th>DM_Z</th>\n",
       "      <th>potential_energy</th>\n",
       "      <th>mulliken_charge_atom_0</th>\n",
       "      <th>mulliken_charge_atom_1</th>\n",
       "      <th>XX_atom_0</th>\n",
       "      <th>YX_atom_0</th>\n",
       "      <th>ZX_atom_0</th>\n",
       "      <th>XY_atom_0</th>\n",
       "      <th>YY_atom_0</th>\n",
       "      <th>ZY_atom_0</th>\n",
       "      <th>XZ_atom_0</th>\n",
       "      <th>YZ_atom_0</th>\n",
       "      <th>ZZ_atom_0</th>\n",
       "      <th>XX_atom_1</th>\n",
       "      <th>YX_atom_1</th>\n",
       "      <th>ZX_atom_1</th>\n",
       "      <th>XY_atom_1</th>\n",
       "      <th>YY_atom_1</th>\n",
       "      <th>ZY_atom_1</th>\n",
       "      <th>XZ_atom_1</th>\n",
       "      <th>YZ_atom_1</th>\n",
       "      <th>ZZ_atom_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8076</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.52368</td>\n",
       "      <td>0.133921</td>\n",
       "      <td>-0.535689</td>\n",
       "      <td>31.3410</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>4.0544</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>28.9546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>4.0546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>34.0861</td>\n",
       "      <td>195.3150</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>195.3170</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>195.3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8074</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.52368</td>\n",
       "      <td>0.133922</td>\n",
       "      <td>-0.535689</td>\n",
       "      <td>31.5814</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>-4.1474</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>28.9036</td>\n",
       "      <td>-1.6036</td>\n",
       "      <td>-4.1476</td>\n",
       "      <td>-1.6036</td>\n",
       "      <td>33.8967</td>\n",
       "      <td>195.3150</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>195.3170</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>195.3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8093</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.52368</td>\n",
       "      <td>0.133923</td>\n",
       "      <td>-0.535689</td>\n",
       "      <td>31.5172</td>\n",
       "      <td>4.1086</td>\n",
       "      <td>1.2723</td>\n",
       "      <td>4.1088</td>\n",
       "      <td>33.9068</td>\n",
       "      <td>1.6950</td>\n",
       "      <td>1.2724</td>\n",
       "      <td>1.6951</td>\n",
       "      <td>28.9579</td>\n",
       "      <td>195.3150</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>195.3170</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>195.3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8095</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.52368</td>\n",
       "      <td>0.133923</td>\n",
       "      <td>-0.535689</td>\n",
       "      <td>31.4029</td>\n",
       "      <td>-4.0942</td>\n",
       "      <td>-1.1793</td>\n",
       "      <td>-4.0944</td>\n",
       "      <td>34.0776</td>\n",
       "      <td>1.6259</td>\n",
       "      <td>-1.1795</td>\n",
       "      <td>1.6260</td>\n",
       "      <td>28.9013</td>\n",
       "      <td>195.3150</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>195.3170</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>195.3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-11.2570</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.52368</td>\n",
       "      <td>0.133921</td>\n",
       "      <td>0.133922</td>\n",
       "      <td>31.3410</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>4.0544</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>28.9546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>4.0546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>34.0861</td>\n",
       "      <td>31.5814</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>-4.1474</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>28.9036</td>\n",
       "      <td>-1.6036</td>\n",
       "      <td>-4.1476</td>\n",
       "      <td>-1.6036</td>\n",
       "      <td>33.8967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_0  atom_index_1  scalar_coupling_constant  \\\n",
       "0   0  dsgdb9nsd_000001             1             0                   84.8076   \n",
       "1   4  dsgdb9nsd_000001             2             0                   84.8074   \n",
       "2   7  dsgdb9nsd_000001             3             0                   84.8093   \n",
       "3   9  dsgdb9nsd_000001             4             0                   84.8095   \n",
       "4   1  dsgdb9nsd_000001             1             2                  -11.2570   \n",
       "\n",
       "   coupling_type_1JHC  coupling_type_1JHN  coupling_type_2JHC  \\\n",
       "0                   1                   0                   0   \n",
       "1                   1                   0                   0   \n",
       "2                   1                   0                   0   \n",
       "3                   1                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   coupling_type_2JHH  coupling_type_2JHN  coupling_type_3JHC  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   1                   0                   0   \n",
       "\n",
       "   coupling_type_3JHH  coupling_type_3JHN  DM_X  DM_Y  DM_Z  potential_energy  \\\n",
       "0                   0                   0   0.0   0.0   0.0         -40.52368   \n",
       "1                   0                   0   0.0   0.0   0.0         -40.52368   \n",
       "2                   0                   0   0.0   0.0   0.0         -40.52368   \n",
       "3                   0                   0   0.0   0.0   0.0         -40.52368   \n",
       "4                   0                   0   0.0   0.0   0.0         -40.52368   \n",
       "\n",
       "   mulliken_charge_atom_0  mulliken_charge_atom_1  XX_atom_0  YX_atom_0  \\\n",
       "0                0.133921               -0.535689    31.3410    -1.2317   \n",
       "1                0.133922               -0.535689    31.5814     1.2173   \n",
       "2                0.133923               -0.535689    31.5172     4.1086   \n",
       "3                0.133923               -0.535689    31.4029    -4.0942   \n",
       "4                0.133921                0.133922    31.3410    -1.2317   \n",
       "\n",
       "   ZX_atom_0  XY_atom_0  YY_atom_0  ZY_atom_0  XZ_atom_0  YZ_atom_0  \\\n",
       "0     4.0544    -1.2317    28.9546    -1.7173     4.0546    -1.7173   \n",
       "1    -4.1474     1.2173    28.9036    -1.6036    -4.1476    -1.6036   \n",
       "2     1.2723     4.1088    33.9068     1.6950     1.2724     1.6951   \n",
       "3    -1.1793    -4.0944    34.0776     1.6259    -1.1795     1.6260   \n",
       "4     4.0544    -1.2317    28.9546    -1.7173     4.0546    -1.7173   \n",
       "\n",
       "   ZZ_atom_0  XX_atom_1  YX_atom_1  ZX_atom_1  XY_atom_1  YY_atom_1  \\\n",
       "0    34.0861   195.3150     0.0000    -0.0001     0.0000   195.3170   \n",
       "1    33.8967   195.3150     0.0000    -0.0001     0.0000   195.3170   \n",
       "2    28.9579   195.3150     0.0000    -0.0001     0.0000   195.3170   \n",
       "3    28.9013   195.3150     0.0000    -0.0001     0.0000   195.3170   \n",
       "4    34.0861    31.5814     1.2173    -4.1474     1.2173    28.9036   \n",
       "\n",
       "   ZY_atom_1  XZ_atom_1  YZ_atom_1  ZZ_atom_1  \n",
       "0     0.0007    -0.0001     0.0007   195.3170  \n",
       "1     0.0007    -0.0001     0.0007   195.3170  \n",
       "2     0.0007    -0.0001     0.0007   195.3170  \n",
       "3     0.0007    -0.0001     0.0007   195.3170  \n",
       "4    -1.6036    -4.1476    -1.6036    33.8967  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No shuffle to the samples, this will be done later\n",
    "train_all = data_load['train']\n",
    "train_all =  pd.get_dummies(train_all, prefix = 'coupling_type', columns = ['type'])\n",
    "train_col = train_all.columns\n",
    "\n",
    "# Add the dipole_moments, per molecule\n",
    "dipole_moments = data_load['dipole_moments'].rename(columns = {\"X\": 'DM_X', \"Y\": \"DM_Y\", \"Z\": \"DM_Z\" })\n",
    "train_all = train_all.merge(dipole_moments, on = ['molecule_name'])\n",
    "\n",
    "# Add the potential energy, per molecule\n",
    "potential_energy = data_load['potential_energy']\n",
    "train_all = train_all.merge(potential_energy, on = ['molecule_name'])\n",
    "\n",
    "\n",
    "# Add the Mulliken charges, per atom\n",
    "mulliken_charges = data_load['mulliken_charges']\n",
    "train_all = train_all.merge(mulliken_charges, \n",
    "                            left_on = ['molecule_name', 'atom_index_0'], \n",
    "                            right_on = ['molecule_name', 'atom_index'] )\n",
    "train_all = train_all.merge(mulliken_charges, \n",
    "                            left_on = ['molecule_name', 'atom_index_1'], \n",
    "                            right_on = ['molecule_name', 'atom_index'], suffixes = ('_atom_0', '_atom_1') \n",
    "                           ).drop(['atom_index_atom_0', 'atom_index_atom_1'], axis = 1)\n",
    "\n",
    "# Add magnetic shileding, per atom\n",
    "magnetic_shielding_tensors = data_load['magnetic_shielding_tensors']\n",
    "train_all = train_all.merge(magnetic_shielding_tensors, \n",
    "                            left_on = ['molecule_name', 'atom_index_0'], \n",
    "                            right_on = ['molecule_name', 'atom_index'] )\n",
    "train_all = train_all.merge(magnetic_shielding_tensors, \n",
    "                            left_on = ['molecule_name', 'atom_index_1'], \n",
    "                            right_on = ['molecule_name', 'atom_index'], suffixes = ('_atom_0', '_atom_1') \n",
    "                           ).drop(['atom_index_atom_0', 'atom_index_atom_1'], axis = 1)\n",
    "\n",
    "\n",
    "train_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: besides train features, data are not provided in the test dataset.\n",
    "\n",
    "Should we use those as generative features for the scalar coupling constant?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to prepare the global set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import re\n",
    "\n",
    "\n",
    "def create_coupling_per_mol(molecules, coupled_atoms):\n",
    "    \n",
    "    #0 Select all molecules in the batch first (to quicken the operations)\n",
    "    molecules_name = coupled_atoms[['molecule_name']].values.reshape(-1)   \n",
    "    molecules = molecules.loc[molecules['molecule_name'].isin(molecules_name)]\n",
    "\n",
    "    #2 Prepare the features\n",
    "    coupled_atoms.drop(['id'], axis = 1, inplace = True)\n",
    "\n",
    "    coupling_id_col = ['molecule_name', 'atom_index_0', 'atom_index_1' ] \n",
    "    coupling_col = list(filter(re.compile(\"coupling_type.*\").match,coupled_atoms.columns))\n",
    "    outputs_atom_0_col = list(filter(re.compile(\".*_atom_0\").match,coupled_atoms.columns))\n",
    "    outputs_atom_1_col = list(filter(re.compile(\".*_atom_1\").match,coupled_atoms.columns))\n",
    "    outputs_mol_col = ['DM_X','DM_Y','DM_Z','potential_energy']\n",
    "    \n",
    "    #mean_col = ['x_mean', 'y_mean', 'z_mean']\n",
    "    \n",
    "    # Add type and coordinates for each atom_index_0 and atom_index_1\n",
    "    df_tmp = coupled_atoms.merge(molecules, how='inner', \n",
    "                               left_on=['molecule_name', 'atom_index_0'], \n",
    "                               right_on=['molecule_name', 'atom_index'])\n",
    "    df_features_dnn = df_tmp.merge(molecules, how='inner', \n",
    "                                          left_on=['molecule_name', 'atom_index_1'], \n",
    "                                          right_on=['molecule_name', 'atom_index'],\n",
    "                                         suffixes=('_fa0', '_fa1')).drop(['atom_index_fa0','atom_index_fa1' ], axis = 1)\n",
    "    \n",
    "    # Return a tuple with 5 components grouped by molecule, in molecule order\n",
    "    gb = df_features_dnn.groupby(['molecule_name'], sort = True, group_keys = False, as_index = True)\n",
    "    \n",
    "    coupling_pos_col = ['x_fa0' ,'y_fa0','z_fa0', 'x_fa1' ,'y_fa1','z_fa1']\n",
    "    coupling_per_mol = gb.apply(lambda frame: frame[coupling_col + coupling_pos_col].to_numpy()).to_list()\n",
    "    outputs_0 = gb.apply(lambda frame: frame[outputs_atom_0_col + outputs_mol_col].to_numpy()).to_list()\n",
    "    outputs_1 = gb.apply(lambda frame: frame[outputs_atom_1_col + outputs_mol_col].to_numpy()).to_list()\n",
    "    y_per_mol = gb.apply(lambda frame: frame['scalar_coupling_constant'].to_numpy()).to_list()\n",
    "    \n",
    "    # Return the molecule info per molecule, sorted by molecule order\n",
    "    gb = molecules.groupby(['molecule_name'], sort = True, group_keys = False, as_index = True)\n",
    "    molecules_output = gb.apply(lambda frame: frame.drop(['molecule_name', 'atom_index'], axis = 1).to_numpy()).to_list()\n",
    "    \n",
    "    return coupling_per_mol, y_per_mol, outputs_0, outputs_1, molecules_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the global train/eval sets (about 5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "\n",
    "if CREATE_SET:\n",
    "\n",
    "    validation_ratio = 0.2\n",
    "\n",
    "    # Retrieve the full list of molecules in the train/eval set\n",
    "    molecule_list = train_all['molecule_name'].unique()\n",
    "\n",
    "    # Split the train/eval sets on the molecule list\n",
    "    train_mol, eval_mol = np.split(molecule_list, [int((1-validation_ratio)*len(molecule_list))])\n",
    "    start = timer()\n",
    "\n",
    "    # Select the samples belonging to the molecule train set (WARNING: there is no shuffle there!!)\n",
    "    train_molecules = structures[structures['molecule_name'].isin(train_mol)]\n",
    "    train_coupling, train_y, train_outputs_0, train_outputs_1, train_mols = create_coupling_per_mol(\n",
    "                                        train_molecules,\n",
    "                                        train_all.loc[train_all['molecule_name'].isin(train_mol)]\n",
    "                                        )\n",
    "    # Shuffle the samples by molecule\n",
    "    shuffle(train_coupling, train_y, train_outputs_0, train_outputs_1, train_mols)\n",
    "\n",
    "    train_stop = timer()\n",
    "\n",
    "    # Select the samples belonging to the molecule eval set (WARNING: there is no shuffle there!!)\n",
    "    eval_molecules = structures[structures['molecule_name'].isin(eval_mol)]\n",
    "    eval_coupling, eval_y, eval_outputs_0, eval_outputs_1, eval_mols = create_coupling_per_mol(\n",
    "                                        eval_molecules,\n",
    "                                        train_all.loc[train_all['molecule_name'].isin(eval_mol)]\n",
    "                                        )\n",
    "\n",
    "    # Shuffle the samples by molecule\n",
    "    shuffle(eval_coupling, eval_y, eval_outputs_0, eval_outputs_1, eval_mols)\n",
    "\n",
    "    stop = timer()\n",
    "\n",
    "    print('Number of samples: \\nfor train set:', len(train_coupling), '\\nfor eval set:', len(eval_coupling))\n",
    "    print('Time for train / eval:', train_stop-start, '/', stop-train_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To save the global set of molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import pickle\n",
    "\n",
    "if CREATE_SET:\n",
    "    global_data = {\n",
    "        'train_coupling': train_coupling,\n",
    "        'train_y': train_y,\n",
    "        'train_outputs_0': train_outputs_0,\n",
    "        'train_outputs_1': train_outputs_1,\n",
    "        'train_mols': train_mols,\n",
    "        'eval_coupling': eval_coupling,\n",
    "        'eval_y': eval_y,\n",
    "        'eval_outputs_0': eval_outputs_0,\n",
    "        'eval_outputs_1': eval_outputs_1,\n",
    "        'eval_mols': eval_mols,\n",
    "        }\n",
    "\n",
    "    # Step 2\n",
    "    with open('train-eval-dump-2', 'wb') as dump_file:\n",
    "\n",
    "        # Step 3\n",
    "        pickle.dump(global_data, dump_file)\n",
    "        \n",
    "else:\n",
    "    with open('train-eval-dump-2', 'rb') as dump_file:\n",
    "        global_data = pickle.load(dump_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 08:00:37.492140  8180 deprecation.py:323] From D:\\Program_Files\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({input_dnn: (2000, 14), input_0: (2000, None, 8), input_1: (2000, None, 8)}, {y: (2000,), rnn_output_0: (2000, 14), rnn_output_1: (2000, 14)}), types: ({input_dnn: tf.float32, input_0: tf.float32, input_1: tf.float32}, {y: tf.float32, rnn_output_0: tf.float32, rnn_output_1: tf.float32})>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative with masking and large batch (2500)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "MASKING_VALUE = 666\n",
    "\n",
    "@tf.function\n",
    "def treat_molecule_set(coupled_atoms_set, y, output_0, output_1, molecule):\n",
    "    \n",
    "    coupled_atoms_set = tf.convert_to_tensor(coupled_atoms_set)\n",
    "    output_0 = tf.convert_to_tensor(output_0)\n",
    "    output_1 = tf.convert_to_tensor(output_1)\n",
    "    molecule = tf.convert_to_tensor(molecule)\n",
    "    y = tf.convert_to_tensor(y)\n",
    "    \n",
    "    mol_atoms_nb = tf.shape(molecule)[0]\n",
    "    init_molecule = molecule\n",
    "    \n",
    "    # Extend the molecule to 3D and transpose it \n",
    "    molecule = tf.reshape(molecule[:,:3], [-1, 3, 1])\n",
    "    molecule = tf.transpose(molecule, [2, 1, 0])\n",
    "    \n",
    "    # We repeat the molecules and the mean values, so they have the same n_samples * n_atoms dimension\n",
    "    molecule = tf.tile(molecule, [tf.shape(coupled_atoms_set)[0], 1, 1])\n",
    "\n",
    "    pos_0_3d = tf.reshape(coupled_atoms_set[:, -6:-3], [-1, 3, 1])  \n",
    "    pos_0_3d = tf.tile(pos_0_3d, [1, 1, mol_atoms_nb])\n",
    "    \n",
    "    pos_1_3d = tf.reshape(coupled_atoms_set[:, -3:], [-1, 3, 1])  \n",
    "    pos_1_3d = tf.tile(pos_1_3d, [1, 1, mol_atoms_nb])\n",
    "    \n",
    "    \n",
    "    # We calculate the distance \n",
    "    distance_0 = tf.reduce_sum(tf.square(tf.subtract(molecule, pos_0_3d)), 1)\n",
    "    distance_1 = tf.reduce_sum(tf.square(tf.subtract(molecule, pos_1_3d)), 1)\n",
    "    \n",
    "    # This function defines for each sample the atoms of the molecule, sorted by decreasing order\n",
    "    ordered_idx_0 = tf.argsort(\n",
    "        distance_0,\n",
    "        axis=1,\n",
    "        direction='DESCENDING',\n",
    "        stable=False,\n",
    "        name=None\n",
    "    )\n",
    "    \n",
    "    ordered_idx_1 = tf.argsort(\n",
    "        distance_1,\n",
    "        axis=1,\n",
    "        direction='DESCENDING',\n",
    "        stable=False,\n",
    "        name=None\n",
    "    )\n",
    "    \n",
    "    # We apply this sorting order to the initial molecule to calculate, for each couple of atoms, \n",
    "    # the atoms sorted by decreasing distance\n",
    "    sort_mol_0 = tf.gather(\n",
    "        init_molecule,\n",
    "        ordered_idx_0\n",
    "    )\n",
    "    sort_mol_1 = tf.gather(\n",
    "        init_molecule,\n",
    "        ordered_idx_1\n",
    "    )\n",
    "    \n",
    "    # Pad with 666 to keep the same size (is that even needed?)\n",
    "    sort_mol_0 = tf.pad(sort_mol_0, [[0,0],[0,MAX_MOL_ATOMS_NB-mol_atoms_nb],[0,0]], constant_values=MASKING_VALUE)\n",
    "    sort_mol_1 = tf.pad(sort_mol_1, [[0,0],[0,MAX_MOL_ATOMS_NB-mol_atoms_nb],[0,0]], constant_values=MASKING_VALUE)\n",
    "    return { 'input_dnn': coupled_atoms_set, \n",
    "            'input_0': sort_mol_0,\n",
    "            'input_1': sort_mol_1\n",
    "        }, { 'y': y,\n",
    "            'rnn_output_0': output_0,\n",
    "            'rnn_output_1': output_1}\n",
    "\n",
    "coupling = global_data['train_coupling']\n",
    "y  = global_data['train_y']\n",
    "outputs_0 = global_data['train_outputs_0']\n",
    "outputs_1 = global_data['train_outputs_1']\n",
    "mols = global_data['train_mols']\n",
    "\n",
    "\n",
    "def gen_mol():\n",
    "    for i in range(len(coupling)):     \n",
    "        yield coupling[i], y[i], outputs_0[i], outputs_1[i], mols[i]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    gen_mol , (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32), \n",
    "    (\n",
    "        tf.TensorShape([None, 14]), \n",
    "        tf.TensorShape([None,]), \n",
    "        tf.TensorShape([None, 14]), \n",
    "        tf.TensorShape([None, 14]), \n",
    "        tf.TensorShape([None,8])\n",
    "    ))\n",
    "\n",
    "train_dataset = train_dataset.flat_map(\n",
    "        lambda x, y, z0, z1, mol: tf.data.Dataset.from_tensor_slices(treat_molecule_set(x, y, z0, z1, mol))\n",
    "    )\n",
    "train_dataset = train_dataset.batch(2000,\n",
    "    drop_remainder=True)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.prefetch(1)\n",
    "\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({input_dnn: (2000, 14), input_0: (2000, None, 8), input_1: (2000, None, 8)}, {y: (2000,), rnn_output_0: (2000, 14), rnn_output_1: (2000, 14)}), types: ({input_dnn: tf.float32, input_0: tf.float32, input_1: tf.float32}, {y: tf.float32, rnn_output_0: tf.float32, rnn_output_1: tf.float32})>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative with batch of 1000\n",
    "\n",
    "coupling_eval = global_data['eval_coupling']\n",
    "y_eval  = global_data['eval_y']\n",
    "outputs_0_eval = global_data['eval_outputs_0']\n",
    "outputs_1_eval = global_data['eval_outputs_1']\n",
    "mols_eval = global_data['eval_mols']\n",
    "\n",
    "def gen_mol_eval():\n",
    "    for i in range(len(coupling_eval)):     \n",
    "        yield coupling_eval[i], y_eval[i], outputs_0_eval[i], outputs_1_eval[i], mols_eval[i]\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_generator(\n",
    "    gen_mol_eval , (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32), \n",
    "    (\n",
    "        tf.TensorShape([None, 14]), \n",
    "        tf.TensorShape([None,]), \n",
    "        tf.TensorShape([None, 14]), \n",
    "        tf.TensorShape([None, 14]), \n",
    "        tf.TensorShape([None,8])\n",
    "    ))\n",
    "\n",
    "eval_dataset = eval_dataset.flat_map(lambda x, y, z0, z1, mol: tf.data.Dataset.from_tensor_slices(treat_molecule_set(x, y, z0, z1, mol)))\n",
    "eval_dataset = eval_dataset.batch(2000,\n",
    "    drop_remainder=True)\n",
    "\n",
    "eval_dataset = eval_dataset.repeat()\n",
    "eval_dataset = eval_dataset.prefetch(1)\n",
    "\n",
    "eval_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0716 08:00:38.559914  8180 deprecation.py:323] From D:\\Program_Files\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3868: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_global\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_dnn (InputLayer)          [(None, 14)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 14)           56          input_dnn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           480         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_0 (InputLayer)            [(None, 29, 8)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 29, 8)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "model_rnn (Model)               (None, 32)           13568       input_0[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           1056        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96)           0           model_rnn[1][0]                  \n",
      "                                                                 model_rnn[2][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 96)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           1056        model_rnn[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           1056        model_rnn[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           3104        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "y (Dense)                       (None, 1)            33          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "rnn_output_0 (Dense)            (None, 14)           462         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "rnn_output_1 (Dense)            (None, 14)           462         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,333\n",
      "Trainable params: 21,305\n",
      "Non-trainable params: 28\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "MASKING_VALUE = 666\n",
    "\n",
    "input_dnn = keras.Input(shape =(14,), name='input_dnn' )\n",
    "input_0 = keras.Input(shape = ( MAX_MOL_ATOMS_NB, 8), name = 'input_0' )\n",
    "input_1 = keras.Input(shape = ( MAX_MOL_ATOMS_NB, 8), name = 'input_1' )\n",
    "input_rnn = keras.Input(shape = ( MAX_MOL_ATOMS_NB, 8))\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    dnn = layers.BatchNormalization()(input_dnn)\n",
    "    dnn = layers.Dense(32, activation='relu')(dnn)\n",
    "    dnn = layers.Dropout(0.5)(dnn)\n",
    "    dnn_output = layers.Dense(32, activation='relu')(dnn)\n",
    "\n",
    "    # Create a model for the rnn, shared for both atoms 0 and 1\n",
    "    \n",
    "    rnn = layers.Masking(mask_value=MASKING_VALUE, input_shape=(MAX_MOL_ATOMS_NB, 8))(input_rnn) \n",
    "    rnn = layers.LSTM(32, return_sequences=True, dropout = 0.5)(rnn)\n",
    "    rnn_encoding = layers.LSTM(32, return_sequences=False, dropout = 0.5)(rnn)\n",
    "    rnn_output = layers.Dropout(0.5)(rnn_encoding)\n",
    "\n",
    "    model_rnn = keras.Model(input_rnn, outputs = [rnn_output], name = 'model_rnn')\n",
    "\n",
    "    rnn_encoding_0 = model_rnn(input_0)\n",
    "    rnn_output_0 = layers.Dense(32, activation='relu')(rnn_encoding_0)\n",
    "    rnn_output_0 = layers.Dropout(0.5)(rnn_output_0)\n",
    "    output_0 = layers.Dense(14, activation='linear', name = 'rnn_output_0')(rnn_output_0)\n",
    "    \n",
    "    rnn_encoding_1 = model_rnn(input_1)\n",
    "    rnn_output_1 = layers.Dense(32, activation='relu')(rnn_encoding_1)\n",
    "    rnn_output_1 = layers.Dropout(0.5)(rnn_output_1)\n",
    "    output_1 = layers.Dense(14, activation='linear', name = 'rnn_output_1')(rnn_output_1)\n",
    "\n",
    "    y_all = layers.concatenate([rnn_encoding_0, rnn_encoding_1, dnn_output])\n",
    "    y_all = layers.Dropout(0.5)(y_all)\n",
    "    y_all = layers.Dense(32, activation='relu')(y_all)\n",
    "    y_output = layers.Dense(1, activation='linear', name = 'y')(y_all)\n",
    "\n",
    "    return keras.Model(inputs = [input_dnn, input_0, input_1], outputs = [y_output, output_0, output_1], name = 'model_global')\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Customer score to calculate the competition score\n",
    "\n",
    "def custom_loss_wrapper(input_tensor):\n",
    "    \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        \n",
    "        # Coupling one-hot\n",
    "        c = tf.slice(input_tensor, [0,0], [-1,8])\n",
    "        \n",
    "        # without explicit broadcasting\n",
    "        absolute_error_pred = tf.math.abs(tf.subtract(y_true,y_pred))\n",
    "        absolute_error_per_type = tf.multiply(c, absolute_error_pred)\n",
    "        \n",
    "        zerocount_per_type = tf.math.count_nonzero(absolute_error_per_type, axis = 0)\n",
    "        sum_per_type = tf.math.reduce_sum(absolute_error_per_type, axis = 0) \n",
    "        \n",
    "        zerocount_per_type = tf.boolean_mask(zerocount_per_type, tf.greater(zerocount_per_type,0))\n",
    "        sum_per_type = tf.boolean_mask(sum_per_type, tf.greater(sum_per_type,0.0))\n",
    "        \n",
    "        mean_per_type = sum_per_type / tf.cast(zerocount_per_type, tf.float32)\n",
    "        mean_per_type = tf.math.log(mean_per_type)\n",
    "        score = tf.math.reduce_mean(mean_per_type)     \n",
    "        return score\n",
    "    \n",
    "    return custom_loss\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = \".\\\\models\\\\weights-improvement-2019715-34-1242.13.hdf5\"\n",
    "model.load_weights(model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 2967.6295 - y_loss: 1.3828 - rnn_output_0_loss: 665.8200 - rnn_output_1_loss: 2163.5315\n",
      "Epoch 00001: val_loss improved from inf to 1675.13277, saving model to .\\models\\weights-improvement-2019716-01-1675.13.hdf5\n",
      "100/100 [==============================] - 226s 2s/step - loss: 2966.5793 - y_loss: 1.3812 - rnn_output_0_loss: 666.1855 - rnn_output_1_loss: 2162.2695 - val_loss: 1675.1328 - val_y_loss: 1.0872 - val_rnn_output_0_loss: 302.6292 - val_rnn_output_1_loss: 1263.7794\n",
      "Epoch 2/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 2587.3625 - y_loss: 1.2559 - rnn_output_0_loss: 613.2243 - rnn_output_1_loss: 1848.5436\n",
      "Epoch 00002: val_loss did not improve from 1675.13277\n",
      "100/100 [==============================] - 206s 2s/step - loss: 2587.9632 - y_loss: 1.2559 - rnn_output_0_loss: 613.4869 - rnn_output_1_loss: 1848.8910 - val_loss: 1774.5339 - val_y_loss: 1.1188 - val_rnn_output_0_loss: 387.8045 - val_rnn_output_1_loss: 1274.8456\n",
      "Epoch 3/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 2438.8437 - y_loss: 1.2526 - rnn_output_0_loss: 583.6412 - rnn_output_1_loss: 1729.9427\n",
      "Epoch 00003: val_loss did not improve from 1675.13277\n",
      "100/100 [==============================] - 207s 2s/step - loss: 2436.0166 - y_loss: 1.2531 - rnn_output_0_loss: 583.2226 - rnn_output_1_loss: 1727.4868 - val_loss: 1899.8844 - val_y_loss: 1.1554 - val_rnn_output_0_loss: 465.5694 - val_rnn_output_1_loss: 1318.7710\n",
      "Epoch 4/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 3619.9972 - y_loss: 1.4441 - rnn_output_0_loss: 747.2463 - rnn_output_1_loss: 2728.3459\n",
      "Epoch 00004: val_loss improved from 1675.13277 to 1382.08035, saving model to .\\models\\weights-improvement-2019716-04-1382.08.hdf5\n",
      "100/100 [==============================] - 207s 2s/step - loss: 3616.1567 - y_loss: 1.4462 - rnn_output_0_loss: 748.3740 - rnn_output_1_loss: 2723.1584 - val_loss: 1382.0804 - val_y_loss: 1.0193 - val_rnn_output_0_loss: 82.1299 - val_rnn_output_1_loss: 1198.0155\n",
      "Epoch 5/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 3124.4556 - y_loss: 1.4372 - rnn_output_0_loss: 732.7654 - rnn_output_1_loss: 2247.9680\n",
      "Epoch 00005: val_loss did not improve from 1382.08035\n",
      "100/100 [==============================] - 208s 2s/step - loss: 3127.3352 - y_loss: 1.4361 - rnn_output_0_loss: 731.7266 - rnn_output_1_loss: 2251.9985 - val_loss: 1529.2920 - val_y_loss: 1.2557 - val_rnn_output_0_loss: 164.5212 - val_rnn_output_1_loss: 1239.1979\n",
      "Epoch 6/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 2673.4741 - y_loss: 1.2446 - rnn_output_0_loss: 657.4750 - rnn_output_1_loss: 1891.5349\n",
      "Epoch 00006: val_loss improved from 1382.08035 to 1332.84960, saving model to .\\models\\weights-improvement-2019716-06-1332.85.hdf5\n",
      "100/100 [==============================] - 207s 2s/step - loss: 2671.4897 - y_loss: 1.2443 - rnn_output_0_loss: 657.8339 - rnn_output_1_loss: 1889.2233 - val_loss: 1332.8496 - val_y_loss: 1.1742 - val_rnn_output_0_loss: 146.4559 - val_rnn_output_1_loss: 1068.9698\n",
      "Epoch 7/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 2685.7571 - y_loss: 1.2550 - rnn_output_0_loss: 712.7416 - rnn_output_1_loss: 1847.5112\n",
      "Epoch 00007: val_loss improved from 1332.84960 to 1214.56788, saving model to .\\models\\weights-improvement-2019716-07-1214.57.hdf5\n",
      "100/100 [==============================] - 207s 2s/step - loss: 2688.4607 - y_loss: 1.2567 - rnn_output_0_loss: 713.3150 - rnn_output_1_loss: 1849.4719 - val_loss: 1214.5679 - val_y_loss: 1.1730 - val_rnn_output_0_loss: 70.4163 - val_rnn_output_1_loss: 1026.8523\n",
      "Epoch 8/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 3064.2788 - y_loss: 1.3886 - rnn_output_0_loss: 781.5208 - rnn_output_1_loss: 2143.9026\n",
      "Epoch 00008: val_loss improved from 1214.56788 to 1206.66559, saving model to .\\models\\weights-improvement-2019716-08-1206.67.hdf5\n",
      "100/100 [==============================] - 207s 2s/step - loss: 3064.8916 - y_loss: 1.3881 - rnn_output_0_loss: 782.6445 - rnn_output_1_loss: 2143.4409 - val_loss: 1206.6656 - val_y_loss: 1.1341 - val_rnn_output_0_loss: 56.6038 - val_rnn_output_1_loss: 1036.6536\n",
      "Epoch 9/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 3075.0338 - y_loss: 1.3478 - rnn_output_0_loss: 756.1981 - rnn_output_1_loss: 2184.0508\n",
      "Epoch 00009: val_loss improved from 1206.66559 to 1203.98114, saving model to .\\models\\weights-improvement-2019716-09-1203.98.hdf5\n",
      "100/100 [==============================] - 206s 2s/step - loss: 3069.5815 - y_loss: 1.3454 - rnn_output_0_loss: 756.0279 - rnn_output_1_loss: 2179.0085 - val_loss: 1203.9811 - val_y_loss: 1.1467 - val_rnn_output_0_loss: 88.0612 - val_rnn_output_1_loss: 1001.2463\n",
      "Epoch 10/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 2753.5614 - y_loss: 1.2155 - rnn_output_0_loss: 732.7855 - rnn_output_1_loss: 1899.2220\n",
      "Epoch 00010: val_loss did not improve from 1203.98114\n",
      "100/100 [==============================] - 207s 2s/step - loss: 2750.7080 - y_loss: 1.2136 - rnn_output_0_loss: 732.6636 - rnn_output_1_loss: 1896.6797 - val_loss: 1264.3560 - val_y_loss: 1.1794 - val_rnn_output_0_loss: 130.8797 - val_rnn_output_1_loss: 1015.5399\n",
      "Epoch 11/35\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 2636.7939 - y_loss: 1.2412 - rnn_output_0_loss: 705.2413 - rnn_output_1_loss: 1807.4298\n",
      "Epoch 00011: val_loss did not improve from 1203.98114\n",
      "100/100 [==============================] - 206s 2s/step - loss: 2635.3615 - y_loss: 1.2412 - rnn_output_0_loss: 704.6949 - rnn_output_1_loss: 1806.5507 - val_loss: 1288.6638 - val_y_loss: 1.2084 - val_rnn_output_0_loss: 129.7847 - val_rnn_output_1_loss: 1038.0387\n",
      "Epoch 12/35\n",
      " 28/100 [=======>......................] - ETA: 2:16 - loss: 2595.2275 - y_loss: 1.2277 - rnn_output_0_loss: 696.6935 - rnn_output_1_loss: 1775.7598"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "adam = keras.optimizers.Adam()\n",
    "model.compile(loss=[custom_loss_wrapper(input_dnn), 'mean_squared_error', 'mean_squared_error'], \n",
    "              loss_weights = [100,1,1], optimizer=adam)\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "#logdir=\".\\\\logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# checkpoint\n",
    "filepath=\".\\\\models\\\\weights-improvement-\" + str(now.year) + str(now.month) + str(now.day) + \"-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_dataset, epochs=35, validation_data=eval_dataset, \n",
    "                    steps_per_epoch = 100, validation_steps=200, callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit --port 8088\n",
    "#http://g751-antoine:8088/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/ \n",
    "%tensorboard --logdir logs/fit --port 8088\n",
    "#http://g751-antoine:8088/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = \".\\\\models\\\\weights-final-2019712-04-31.99.hdf5\"\n",
    "model.save_weights(model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%kill 8168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "MASKING_VALUE = 666\n",
    "\n",
    "input_dnn = keras.Input(shape =(14,), name='input_dnn' )\n",
    "\n",
    "def create_simple_model():\n",
    "\n",
    "    dnn = layers.BatchNormalization()(input_dnn)\n",
    "    dnn = layers.Dense(32, activation='relu')(dnn)\n",
    "    dnn = layers.Dropout(0.5)(dnn)\n",
    "    dnn_output = layers.Dense(32, activation='relu')(dnn)\n",
    "    y_all = layers.Dropout(0.5)(dnn_output)\n",
    "    y_all = layers.Dense(32, activation='relu')(y_all)\n",
    "    y_all = layers.Dropout(0.5)(y_all)\n",
    "    y_output = layers.Dense(1, activation='linear', name = 'y')(y_all)\n",
    "    \n",
    "    return keras.Model(inputs = [input_dnn], outputs = [y_output], name = 'simple_model')\n",
    "\n",
    "simple_model = create_simple_model()\n",
    "\n",
    "def custom_loss_wrapper(input_tensor):\n",
    "    \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        \n",
    "        # Coupling one-hot\n",
    "        c = tf.slice(input_tensor, [0,0], [-1,8])\n",
    "        \n",
    "        # without explicit broadcasting\n",
    "        absolute_error_pred = tf.math.abs(tf.subtract(y_true,y_pred))\n",
    "        absolute_error_per_type = tf.multiply(c, absolute_error_pred)\n",
    "        \n",
    "        zerocount_per_type = tf.math.count_nonzero(absolute_error_per_type, axis = 0)\n",
    "        sum_per_type = tf.math.reduce_sum(absolute_error_per_type, axis = 0) \n",
    "        \n",
    "        zerocount_per_type = tf.boolean_mask(zerocount_per_type, tf.greater(zerocount_per_type,0))\n",
    "        sum_per_type = tf.boolean_mask(sum_per_type, tf.greater(sum_per_type,0.0))\n",
    "        \n",
    "        mean_per_type = sum_per_type / tf.cast(zerocount_per_type, tf.float32)\n",
    "        mean_per_type = tf.math.log(mean_per_type)\n",
    "        score = tf.math.reduce_mean(mean_per_type)     \n",
    "        return score\n",
    "    \n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "adam = keras.optimizers.Adam()\n",
    "simple_model.compile(loss=[custom_loss_wrapper(input_dnn)], optimizer=adam)\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\".\\\\logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"./models/weights-improvement-\" + str(now.year) + str(now.month) + str(now.day) + \"-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "# Fit the model\n",
    "history = simple_model.fit(train_dataset, epochs=5, validation_data=eval_dataset, \n",
    "                    steps_per_epoch = 500, validation_steps=100, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'eval'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions: \n",
    "* how do we bring the dataset to the model with several inputs, and outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.\n",
    "\n",
    "score=1Tt=1Tlog(1nti=1nt|yiyi^|)\n",
    "Where:\n",
    "\n",
    "T is the number of scalar coupling types\n",
    "nt is the number of observations of type t\n",
    "yi is the actual scalar coupling constant for the observation\n",
    "yi^ is the predicted scalar coupling constant for the observation\n",
    "For this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dipole_moments.csv - contains the molecular electric dipole moments. These are three dimensional vectors that indicate the charge distribution in the molecule. The first column (molecule_name) are the names of the molecule, the second to fourth column are the X, Y and Z components respectively of the dipole moment.\n",
    "* magnetic_shielding_tensors.csv - contains the magnetic shielding tensors for all atoms in the molecules. The first column (molecule_name) contains the molecule name, the second column (atom_index) contains the index of the atom in the molecule, the  third to eleventh columns contain the XX, YX, ZX, XY, YY, ZY, XZ, YZ and ZZ elements of the tensor/matrix respectively.\n",
    "* mulliken_charges.csv - contains the mulliken charges for all atoms in the molecules. The first column (molecule_name) contains the name of the molecule, the second column (atom_index) contains the index of the atom in the molecule, the third column (mulliken_charge) contains the mulliken charge of the atom.\n",
    "* potential_energy.csv - contains the potential energy of the molecules. The first column (molecule_name) contains the name of the molecule, the second column (potential_energy) contains the potential energy of the molecule.\n",
    "* scalar_coupling_contributions.csv - The scalar coupling constants in train.csv (or corresponding files) are a sum of four terms. scalar_coupling_contributions.csv contain all these terms. The first column (molecule_name) are the name of the molecule, the second (atom_index_0) and third column (atom_index_1) are the atom indices of the atom-pair, the fourth column indicates the type of coupling, the fifth column (fc) is the Fermi Contact contribution, the sixth column (sd) is the Spin-dipolar contribution, the seventh column (pso) is the Paramagnetic spin-orbit contribution and the eighth column (dso) is the Diamagnetic spin-orbit contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set to validate the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ut_coupling = pd.DataFrame({\n",
    "    'molecule_name': ['mol_a', 'mol_b', 'mol_a', 'mol_b', 'mol_b', 'mol_c'],\n",
    "    'atom_index_0':[ 0, 0, 1, 1, 1, 0],\n",
    "    'atom_index_1': [ 1, 1, 2, 2, 3, 1],\n",
    "    'scalar_coupling_constant': [5, 7, 6, 8, 9, 10],\n",
    "    'coupling_type_1JHC':[ 1, 0, 0, 0, 0, 0], \n",
    "    'coupling_type_1JHN':[ 0, 1, 0, 0, 0, 0], \n",
    "    'coupling_type_2JHC':[ 0, 0, 1, 0, 0, 0], \n",
    "    'coupling_type_2JHH':[ 0, 0, 0, 1, 0, 0], \n",
    "    'coupling_type_2JHN':[ 0, 0, 0, 0, 1, 0], \n",
    "    'coupling_type_3JHC':[ 0, 0, 0, 0, 0, 1],\n",
    "    'coupling_type_3JHH':[ 0, 0, 0, 0, 0, 0], \n",
    "    'coupling_type_3JHN':[ 0, 0, 0, 0, 0, 0]\n",
    "})\n",
    "\n",
    "ut_coupling.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_molecules = pd.DataFrame({\n",
    "    'molecule_name': ['mol_a', 'mol_a', 'mol_a', 'mol_b', 'mol_b', 'mol_b', 'mol_b', 'mol_b', 'mol_c', 'mol_c'],\n",
    "    'atom_index':[ 0, 1, 2, 0, 1, 2, 3, 4, 0, 1],\n",
    "    'x': [ -0.5, 0.5, 1, -2, -1, 0, 1, 2, 0.5, 1.5  ],\n",
    "    'y': [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0  ],\n",
    "    'z': [ -0.5, 0.5, 1, -2, -1, 0, 1, 2, 0.5, 1.5  ],\n",
    "    'atom_C': [1,0,0,0,0,1,0,0,0,0],\n",
    "    'atom_F': [0,1,0,0,0,0,1,0,0,0],\n",
    "    'atom_H': [0,0,1,0,0,0,0,1,0,0], \n",
    "    'atom_N': [0,0,0,1,0,0,0,0,1,0],\n",
    "    'atom_O': [0,0,0,0,1,0,0,0,0,1]\n",
    "})\n",
    "\n",
    "ut_molecules.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
